# Teaching Log — Week 1: Haiti Earthquake (Simulated)

**Teacher:** Ms Folake Adeyemi
**Class:** Year 8, Set 3 (mixed attainment, 30 pupils)
**Week:** Spring Term, Week 4
**Vehicle:** GE-KS3-CV001 — Haiti 2010 Earthquake
**Contrasting vehicle:** GE-KS3-CV002 — Japan 2011 (previewed this week, fully deployed next week)
**Primary Thinking Lenses:** Systems and System Models (L1-L2), Cause and Effect (L3-L4), Evidence and Argument (L5)

---

## Monday — Lesson 1: The Earth Machine

### What happened

Started with the Systems and System Models lens question written on the board. Approximately half the class had no idea what "system" meant in a scientific context. I had to backtrack and define it: "A system is a set of connected parts that work together. A bicycle is a system. A school is a system. The Earth is a system." Once I got that across, the idea of Earth having interacting layers clicked faster than usual.

The egg analogy worked as always, but the push beyond it — "an egg is static, the Earth is dynamic" — landed well with about a third of the class. The rest needed the animated convection current diagram before they understood that the layers interact rather than just sitting on top of each other.

### How the vehicle data shaped the lesson

I only used CV001 as a teaser — showed Port-au-Prince on the world map and said "This city was destroyed. By Friday, you will be able to explain why." Several pupils immediately Googled it (we allow phones for research). One pupil said "Miss, was it an earthquake?" I said "How do you know?" She said "Haiti is near the Caribbean." That is exactly the kind of locational knowledge connection (GE-KS3-D001) that the graph models.

I used the USGS data source (from CV001 data_sources) to display the real earthquake epicentre map. The patterns were immediately visible — earthquakes cluster along plate boundaries. I did not tell them this; I asked them what they noticed. Three pupils independently identified the Ring of Fire without knowing the term. This is the Patterns lens working even though I was officially using Systems.

### What the Thinking Lens did

The Systems lens question — "What are all the parts, how do they interact?" — gave the lesson a structure beyond "learn these facts about the Earth." Pupils were not just memorising layers; they were building a model of how those layers cause things to happen on the surface. One pupil said "So the mantle makes the plates move, and the plates make earthquakes?" That is systems thinking. He would not have said that without the framing question.

However, the formal language of the lens ("What happens when something changes?") is too abstract for some Year 8 pupils. I rephrased it as "What happens when the convection current pushes the plate?" Concrete beats abstract with 12-year-olds.

### What I would change

The atlas task took longer than planned. I gave them a blank world map and asked them to locate and label all the major plates. Some pupils cannot locate Europe confidently, let alone the Juan de Fuca Plate. Next time I would give them a partially labelled map and ask them to complete 5 specific plates only.

### Maps used

- World tectonic plate map (from atlas + USGS data_sources) -- displayed on screen
- Earthquake epicentre map (from USGS) -- displayed on screen
- Blank world map for pupil annotation

---

## Tuesday — Lesson 2: Where Plates Meet

### What happened

The three boundary types lesson. This is always challenging because pupils have to hold three abstract processes in their heads simultaneously. I used the approach from the lesson plan: spend extra time on the conservative boundary because that is Haiti's specific setting.

The Stability and Change lens question — "What keeps the surface stable, and what causes sudden change?" — was genuinely useful here. I set up a physical model with two books on a desk, pushing them past each other. They slide smoothly most of the time (stable). Then they catch and jerk (change). "That jerk is an earthquake." Every pupil in the room understood. The lens framing turned what could be a dry classification exercise into a genuine conceptual question about why earthquakes happen suddenly rather than continuously.

### How the vehicle data shaped the lesson

I introduced Haiti's specific tectonic setting: the Enriquillo-Plantain Garden fault zone, a conservative (transform) boundary. The CV001 definitions include "conservative boundary" which confirmed I should use this specific terminology. I also previewed the subduction zone for Japan (CV002) because next week's lesson needs pupils to understand that different boundary types produce different hazards. The contrasting_with relationship between CV001 and CV002 means I need to build the comparison framework from the start.

One pupil asked: "If Japan's earthquake was bigger, why did more people die in Haiti?" I said "Excellent question. We will answer that on Thursday." This is the pedagogical power of the contrasting case structure — it generates questions naturally.

### What the Thinking Lens did

Stability and Change was the right secondary lens for this content. The concept of stress accumulation and sudden release is precisely a stability/change dynamic. Pupils could explain, by the end of the lesson, that the Earth's surface seems stable but that stress is building at boundaries, and that the release of that stress is what we call an earthquake. One pupil wrote in her book: "The plates are always moving but you can't feel it because the crust is stable. It's only at the edges where it's not stable that you get earthquakes." That is geographically accurate and would not have come from a lesson framed as "learn the three types of boundary."

### What I would change

I underestimated how long the subduction explanation would take. Japan's subduction zone is more complex than Haiti's conservative boundary, and pupils struggled with the idea of one plate going under another. I think I should teach this as a full lesson (which is what I have planned for Lesson 6 next week). The preview was too compressed.

### Maps used

- Plate boundary map — showing boundary types colour-coded (projected from atlas)
- Close-up of Caribbean Plate boundary (annotated on board for Haiti)
- Close-up of Japan Trench area (briefly, for preview)

---

## Wednesday — Lesson 3: 12 January 2010 — The Haiti Earthquake

### What happened

This was the lesson I had been building towards. Full deployment of CV001 data.

I started by showing Port-au-Prince on Google Earth — zooming in from the tectonic plate map to the street level. Then I told them: "At 4:53pm on 12 January 2010, the ground shook for 35 seconds. When it stopped, this is what it looked like." I showed before-and-after satellite images (publicly available from Google Earth archive). The visual impact was immediate. Several pupils gasped. One said "Miss, where did the buildings go?"

### How the vehicle data worked in practice

**The data_points were the backbone of the lesson.** I wrote all five on the board:
- Magnitude 7.0
- 230,000+ deaths
- 1.5 million displaced
- GDP $7 billion
- HDI rank 168/189

Then I asked pupils to annotate each one: "What does this number tell us about the earthquake AND about Haiti?" The magnitude tells us about the physical event. The death toll tells us about the impact. The GDP and HDI tell us about the country. One pupil said "The GDP is really low — that's less than my auntie's town probably." (It is roughly equivalent to the GDP of a medium-sized English city. He was not far off.)

**The distinction between physical data (magnitude) and human data (GDP, HDI) was powerful.** It set up Thursday's lesson about vulnerability without me having to force the connection. The data_points in CV001 are deliberately structured to include both physical and developmental indicators, and this dual structure mirrors the dual analysis I want pupils to perform.

**The map_types from CV001 worked well.** I used:
- Tectonic plate boundary map: to show the fault beneath Port-au-Prince
- Population density map: to show 2.5 million people concentrated in a small area
- GIS hazard overlay: I used a USGS ShakeMap (intensity map) overlaid with population data. This was the first time many pupils had seen GIS used to combine different data layers. One pupil said "It's like putting tracing paper over a map" — which is exactly the analogy I use for GIS.

I did NOT use the aid distribution map (that is for Lesson 5).

### What the Thinking Lens did

The Cause and Effect lens question — "What caused this to happen, and how do we know?" — structured the writing activity. Pupils used the "if... then... because..." construction from the Cause and Effect agent_prompt: "If the tectonic plates move along a conservative boundary, then stress builds up at the fault line, because the plates are locked together. When the stress is released, the earthquake happens."

This construction forced pupils to think mechanistically rather than just stating that "an earthquake happened." Several pupils struggled with the "because" clause — they could describe what happened but not explain the mechanism. I modelled two examples on the board and then asked them to write their own. About 20 out of 30 produced mechanistically sound explanations.

### Data quality note

The CV001 data_point "230,000+ deaths" uses the commonly cited figure from the Haitian government. The actual death toll is disputed — some estimates range from 46,000 to 316,000. I mentioned this to the class because it is important for them to understand that even data is not always certain, especially in a country with poor record-keeping. The graph should probably include a note about data reliability for case study data_points. This is a gap.

### Maps used

- Tectonic plate boundary map of the Caribbean (projected, annotated on board)
- Google Earth — Port-au-Prince before/after
- USGS ShakeMap — intensity overlay
- Population density map of Haiti (from UN OCHA)

---

## Thursday — Lesson 4: Why Haiti, Not Havana?

### What happened

This was the strongest lesson of the week. The vulnerability analysis is the core geographical insight of the entire unit, and the vehicle data provided the scaffolding.

I opened with the puzzle: "Cuba and Haiti share the same tectonic fault zone. Cuba had a magnitude 6.2 earthquake in 1947. Fewer than 10 people died. Haiti had a magnitude 7.0 earthquake in 2010. Over 230,000 people died. Why?" The room was silent for a good ten seconds. Then a pupil said "Cuba is richer." I said "Is it? How do we know?" And we were off.

### How the vehicle data worked in practice

**The themes from CV001 — vulnerability, development, governance, international aid effectiveness — were the structural framework for the lesson.** I did not use all four themes; I focused on vulnerability, development, and governance. (International aid effectiveness is for tomorrow.)

**The card sort activity worked brilliantly.** I made 12 factor cards:
- Physical: magnitude 7.0, shallow depth (13km), proximity to densely populated city, aftershocks
- Human: GDP $7 billion, HDI 168/189, no building codes enforced, political instability, informal settlements on hillsides, no seismic monitoring, deforestation causing landslides, 2.5 million people in Port-au-Prince

Pupils sorted these into "physical" and "human" categories, then ranked the top 3 causes of the high death toll. Every single group ranked at least 2 human factors in their top 3. The physical earthquake was a necessary condition, but the human geography determined the outcome. This is the central argument of hazard geography and the pupils discovered it themselves through the data.

**The GDP and HDI data_points from CV001 were essential.** Without them, the lesson would have been "Haiti is poor" — which is vague and borderline stereotyping. With them, it was "Haiti has a GDP of $7 billion and an HDI rank of 168/189, which means..." The data makes the analysis rigorous rather than impressionistic.

### The cross-subject connection

I made the cross-subject link explicit as planned: "Haiti was a French colony. It was the richest colony in the Caribbean because of sugar plantations worked by enslaved Africans. Haiti won its independence in 1804 — the only successful slave revolt in history. But France forced Haiti to pay 150 million francs in 'compensation' for the loss of its colony. Haiti was paying that debt until 1947. That debt is part of why Haiti was poor in 2010."

This connects GE-KS3-C002 (Development) to HI-KS3-C002 (Empire) via the cross-subject CO_TEACHES link that the graph now models. The rationale in the graph reads: "Colonial history explains current patterns of global development and inequality." I read this out to the class. One pupil said "That's not fair." I said "That is the geographical argument — fairness is a political question, but understanding why is a geographical one."

This was the most engaged the class has been all term.

### What the Thinking Lens did

Cause and Effect drove the analytical structure. The lens question "Is there more than one reason?" pushed pupils beyond single-factor explanations. Without the lens framing, many Year 8 pupils would write "Haiti had a big earthquake so lots of people died." With it, they wrote things like "The earthquake was caused by tectonic plate movement, but the high death toll was caused by Haiti being a very poor country with no building codes and lots of people living in badly-built houses." Multi-factor causation is a geographical thinking skill, and the Cause and Effect lens made it explicit.

### What I would change

The colonial history connection took longer than I expected because pupils had questions. "Why did France charge them?" "How much is 150 million francs?" "Is that why Haiti is still poor?" These are excellent questions and I was glad they asked, but it ate into the card sort time. Next time I would give the colonial context as a brief (3-minute) narrative at the start and save the discussion for the History unit.

### Maps used

- Population density map of Port-au-Prince (showing informal settlements on hillsides)
- Comparison: Haiti GDP vs UK regional GDP map (I made this — it is not in the vehicle, but it should be)

---

## Friday — Lesson 5: Did the Aid Work?

### What happened

I shifted to the Evidence and Argument lens for this lesson because the task is evaluative: assess a claim using evidence. The CV001 assessment_guidance asks: "Can pupils evaluate the effectiveness of international response using data?"

This lesson was harder than I expected. Year 8 pupils are not used to evaluating the effectiveness of international institutions. They default to moral judgements: "Aid is good because it helps people" or "Aid is bad because it does not work." Pushing them to use evidence to support their evaluation was the intellectual challenge.

### How the vehicle data worked in practice

**The aid distribution map** (from CV001 map_types) was the starting point. I showed a map of where international aid agencies were operating in Port-au-Prince 6 months after the earthquake. Then I showed a map of where displaced people were still living in tent camps 3 years later. The mismatch was visible. "If the aid was effective, why are people still in tents?"

**The data_sources from CV001 — UN OCHA, Red Cross — gave me the authoritative sources for the debate.** The Red Cross raised $488 million for Haiti. They promised to build 130,000 homes. They built 6. (This is publicly documented by ProPublica and NPR.) I did not get this from the vehicle — I got it from my own knowledge — but the vehicle's data_sources pointed me to the right institutions.

The cholera outbreak was not in the vehicle data, but it is historically significant: UN peacekeepers introduced cholera to Haiti in October 2010, ultimately killing over 10,000 people. I included it because it is relevant to evaluating the international response. **This is a gap in the vehicle data** — the vehicle lists "international aid effectiveness" as a theme but does not include the cholera outbreak, which is arguably the most important piece of evidence about the ineffectiveness of the aid response.

### Structured debate

I ran the debate as planned: "The international aid response to the Haiti earthquake was effective." Pupils had to argue for or against using specific evidence.

**For:**
- $13 billion in international aid pledged
- Emergency response saved thousands of lives in the first 72 hours
- Temporary shelter provided for 1.5 million displaced people
- Schools and hospitals rebuilt

**Against:**
- 6 houses built by the Red Cross out of 130,000 promised
- Cholera introduced by UN peacekeepers
- Much aid money went to foreign contractors, not Haitians
- 10 years later, many displaced people still in camps

The debate was lively. Pupils were genuinely angry about the Red Cross figure. One said "How can they raise all that money and only build 6 houses?" I said "That is an excellent question, and it is exactly the kind of question a geographer asks: where did the money actually go?"

### What the Thinking Lens did

The Evidence and Argument lens structured the evaluation beautifully. The key question — "What is the evidence, how reliable is it, and what conclusions can it support?" — forced pupils to distinguish between the claim ("aid was effective") and the evidence (the data about what actually happened). The agent_prompt question "What would count as evidence against it?" was particularly useful — it pushed pupils to actively look for counter-evidence rather than just accepting the first piece of information they found.

### Assessment writing

Pupils wrote a 200-word evaluation. I used the CV001 success_criteria as my formative assessment checklist:
- Can they explain the tectonic cause? (Most can — L3 worked)
- Can they analyse vulnerability using physical and human factors? (Most can — L4 worked)
- Can they evaluate aid effectiveness using data? (About half can — this is the hardest skill)
- Can they compare with a contrasting HIC? (Not yet — that is next week with Japan)

The weakest responses described what happened without evaluating effectiveness. The strongest responses made claims, supported them with evidence, and acknowledged counter-evidence — exactly the structure the Evidence and Argument lens promotes.

### Maps used

- Aid distribution map (from CV001 map_types)
- Displaced persons camp map (additional — from UN OCHA)
- I did NOT use the GIS hazard overlay this lesson — it was not relevant to the aid evaluation

---

## Week 1 Reflections — How the Content Vehicles Changed My Teaching

### What worked

1. **The data_points gave the case study substance.** In previous years, I taught Haiti using whatever data I could find online, and it was inconsistent. Having the five key data points pre-selected and ready to deploy meant I could focus on analysis rather than data-gathering. The combination of physical data (magnitude) and development data (GDP, HDI) in the same list is pedagogically deliberate and effective.

2. **The contrasting_with relationship is the most powerful feature in the vehicle layer.** The entire structure of Weeks 1-2 depends on the contrast between Haiti and Japan. This is not just a nice feature — it is the pedagogy. Contrasting case studies are how Geography works. The fact that the graph explicitly pairs CV001 and CV002 means an AI generating content from this graph would automatically produce a comparison unit rather than two isolated case studies. That is a huge improvement from v4, where the case studies did not exist at all.

3. **The map_types list was genuinely useful for lesson planning.** I knew which maps I needed before I started planning. "Tectonic plate boundary map, GIS hazard overlay, population density map, aid distribution map" — these are the four maps a Geography teacher would use for this case study, and having them listed in the vehicle saved me planning time.

4. **The themes structured the analytical focus.** "Vulnerability, development, governance, international aid effectiveness" — these four themes gave me four distinct analytical angles on the same event. I used three in the teaching and one in the assessment. The themes are not just labels; they are analytical frameworks.

5. **The Thinking Lenses gave lessons intellectual rigour.** Without the lenses, I would have taught the same content but framed it less sharply. The explicit questions — "What are all the parts of this system?", "What caused this, and how do we know?", "What is the evidence?" — gave pupils a way into the analysis that is more productive than "learn about the Haiti earthquake."

### What did not work perfectly

1. **The data_points need reliability metadata.** The "230,000+ deaths" figure is disputed. The vehicle does not tell me this. If an AI uses this data to generate a lesson, it will present the figure as fact. The vehicle should include a `data_reliability` property or a note field for each data_point.

2. **The cholera outbreak is missing.** This is a significant omission. The vehicle's theme "international aid effectiveness" cannot be properly evaluated without the cholera data. The data_points should include "Cholera outbreak: 10,000+ deaths, traced to UN peacekeepers."

3. **The vehicle has no local comparator data.** Pupils kept asking "How does Haiti compare to here?" I improvised with UK GDP comparisons. The vehicle should include a "local_comparison" property: "Haiti's GDP of $7 billion is roughly equivalent to the GDP of [example UK city]." This grounds abstract development data in pupils' experience.

4. **The map_types are listed but not linked to actual maps.** The vehicle says "tectonic plate boundary map" but does not provide a URL, a dataset, or a specification. I know where to find these maps because I have been teaching for 14 years. A newly qualified teacher would not. The resources array in CV001 is empty. This needs to be populated.

5. **The definitions list overlaps with the concept vocabulary.** CV001 definitions include "epicentre, magnitude, tectonic plate, conservative boundary, vulnerability, resilience, HDI, aid dependency." The concept GE-KS3-C001 key vocabulary includes "earthquake, volcano, subduction, plate tectonics." There is overlap and there are gaps. The vehicle definitions should explicitly extend (not duplicate) the concept vocabulary.

### How pupils engaged with the data

The engagement was highest when pupils were working with the numbers. The magnitude comparison (7.0 vs 9.1) provoked genuine curiosity — "How can a stronger earthquake kill fewer people?" The GDP comparison ($7 billion vs $5.5 trillion) made the development gap tangible. The HDI ranking (168/189 vs 19/189) gave them a second indicator to triangulate.

Year 8 pupils can work with data when the data is meaningful and the questions are clear. The mistake is giving them data without a purpose. The Cause and Effect lens question — "What caused the difference?" — gave the data a purpose. Every number was evidence in an argument, not just a fact to memorise.

### How the contrasting case framework helped

This is the single most important pedagogical innovation in the vehicle layer. Geography has always used contrasting case studies — Haiti vs Japan is a standard GCSE pairing — but the graph now models this explicitly through the contrasting_with property. This means:

1. An AI generating a tectonic hazards unit would automatically pair these two case studies
2. The data_points for both vehicles are structured to be directly comparable (both include magnitude, death toll, GDP)
3. The comparison IS the analysis — the differences between Haiti and Japan are not a bonus activity, they ARE the geography

Without the contrasting_with relationship, an AI might generate a Haiti lesson and a Japan lesson separately, missing the comparison entirely. With it, the comparison is built into the structure. This is how Geography teachers think, and the graph now models that thinking.

### Overall verdict on the vehicle data

**Rating: 8/10 for Geography case studies.**

The data_points, themes, map_types, and contrasting_with relationship are exactly what I need. The definitions and success_criteria are useful. The assessment_guidance is well-pitched.

The missing pieces are: data reliability metadata, the cholera gap, empty resources arrays, and no local comparison data. These are fixable. The structural design is sound.

Compared to v4, where the case study slots were empty shells with no data, no maps, and no contrasting cases, this is a transformational improvement. I could not have planned this week from v4. I planned it comfortably from v5.

-- Ms Adeyemi, February 2026
