# V5 Staff Room Report -- Group Synthesis

**Date:** 2026-02-23
**Team:** Henderson (Y2 Maths), Okonkwo (Y4 English), Kapoor (Y5 Science), Osei (KS3 Science), Adeyemi (KS3 Geography + History)

---

## The Headline

**The graph now knows WHAT to teach and has started learning HOW to teach it. The architecture is right; the data needs filling in.**

The v4 headline was: "The graph knows WHAT to teach. It does not yet know HOW to teach it." Content Vehicles and Thinking Lenses have moved every teacher's rating upward. The consensus has shifted from "brilliant skeleton, empty in practice" to "usable planning tool with data quality issues." Every teacher independently rated Content Vehicles as the single most important addition since the graph was created. Thinking Lenses were universally praised as a bonus nobody asked for but everyone found useful. However, all five teachers also found data errors -- ranging from wrong book recommendations (Okonkwo) to systematically misassigned concept IDs across 6 of 8 KS3 Science vehicles (Osei). The gap is no longer structural. It is content quality, coverage completeness, and data integrity.

---

## Rating Summary Table

| Teacher | Subject | Structure v4 | Structure v5 | Content Readiness v4 | Content Readiness v5 |
|---------|---------|-------------|-------------|---------------------|---------------------|
| Henderson | Y2 Maths | 7/10 | 8/10 (+1) | 4/10 | 6/10 (+2) |
| Okonkwo | Y4 English | 7/10 | 8/10 (+1) | 4/10 | 7/10 (+3) |
| Kapoor | Y5 Science | 7/10 | 7.5/10 (+0.5) | 5.75/10 (weighted) | 7/10 (+1.25) |
| Osei | KS3 Science | 7/10 | 7.5/10 (+0.5) | 5.25/10 (weighted) | 6.5/10 (+1.25) |
| Adeyemi | KS3 Geography | 6/10 | 7.5/10 (+1.5) | 3/10 | 7/10 (+4) |
| Adeyemi | KS3 History | 3/10 | 6/10 (+3) | 1/10 | 6/10 (+5) |
| **Consensus** | | **6.2** | **7.4 (+1.2)** | **3.7** | **6.6 (+2.9)** |

Content generation readiness has nearly doubled. The largest gains are in subjects that previously had nothing: History (+5), Geography (+4), English (+3). These are the subjects where Content Vehicles filled a complete vacuum.

---

## 1. What Content Vehicles Fixed

All five teachers agree: Content Vehicles are the single most important addition to the graph since its creation.

**What they addressed, by subject:**

- **English (Okonkwo):** "You cannot teach English without texts." The v4 graph had no texts, no genres, no writing outcomes. Content Vehicles now provide suggested texts with specific titles and authors, genre classifications, grammar focus areas linked to text study, writing outcomes with word counts, and success criteria. Okonkwo: "The graph has gone from 'brilliant skeleton with a blank space where the text should be' to 'a planning tool I would actually use.'"

- **Science -- KS2 (Kapoor):** The v4 report listed 10 things an AI needs to generate a fair test investigation and noted the graph could provide only 3 of them. Content Vehicles now provide 9 of 10: independent/dependent/controlled variables, structured equipment lists, recording formats, expected outcomes, and safety notes. The missing item is step-by-step method. Investigation generation jumped from 4/10 to 7/10.

- **Science -- KS3 (Osei):** "No Practical Work Layer" was the v4 headline complaint. Content Vehicles now provide structured investigation data with enquiry types, equipment, variables, recording formats, and assessment criteria. Osei: "For the first time, the graph contains enough structured data to generate a practical lesson that a teacher could walk into a lab and deliver." Practical lesson generation jumped from 2/10 to 7/10.

- **Maths (Henderson):** Three of Henderson's top five v4 complaints were addressed. CPA progression is now structured data (not buried in prose): CV001 specifies "concrete (Dienes blocks) -> pictorial (place value chart, part-whole model, number line) -> abstract (numeral representation)." Manipulatives are listed and queryable. Common errors are specific and accurate. Henderson: "An AI can now produce a structurally correct lesson with appropriate resources and CPA progression, whereas before it could only produce a narrative description."

- **Geography (Adeyemi):** "No case studies. This is the most critical gap for Geography." Ten Content Vehicles now provide case studies with structured data points, themes, contrasting pairs, and map types. The Haiti/Japan contrasting pair (CV001/CV002) rated 9/10 for pairing quality. Geography content generation jumped from 3/10 to 7/10.

- **History (Adeyemi):** "History is structurally present but functionally empty." Five Content Vehicles now provide named primary sources with typed classifications, multiple perspectives per period, key events with dates, and educational resource links. The Holocaust vehicle (CV005) rated 9/10 -- the best individual vehicle across all subjects. History content generation jumped from 1/10 to 6/10.

**What Content Vehicles did NOT fix (consensus):**
- No worked examples in Maths (Henderson: "the single most significant gap for content generation")
- No step-by-step method in Science (Kapoor, Osei)
- No difficulty sub-levels within concepts (Henderson, Osei, Adeyemi)
- No model text content or chapter synopses in English (Okonkwo)

---

## 2. What Thinking Lenses Added

All five teachers rated Thinking Lenses positively. None had requested them in v4, but all found them useful in practice.

**What worked:**

- **Cognitive framing.** Every teacher used the lens key questions as lesson starters. Henderson: "The Patterns lens key question drove genuine mathematical reasoning." Kapoor: "Pupils internalised the 'what caused this?' question within 2 days." Adeyemi: "I could write the key question on the board at the start of every lesson. This gives pupils a cognitive framework before they engage with the content."

- **Cross-subject coherence.** Adeyemi highlighted that using Cause and Effect in Geography ("why did Haiti suffer more?") and then in History ("what caused the Black Death?") builds transferable thinking. The same cognitive vocabulary appears across subjects, which is how NGSS Crosscutting Concepts are designed to work.

- **AI-ready prompts.** Osei: "The agent_prompt on each lens is directly usable. I could see an AI tutor using 'Why is it shaped like that? What would happen if this part were different?' as prompts during a cell structure activity." Okonkwo found that translating the key questions into child-friendly language ("What in the book tells you that?" instead of "What is the evidence?") produced effective comprehension questions.

- **Lens selection.** Primary lens assignments were broadly correct across all subjects. Patterns for number work (Henderson), Evidence and Argument for reading comprehension (Okonkwo), Cause and Effect for investigations (Kapoor), Structure and Function for cells (Osei), Systems and System Models for plate tectonics (Adeyemi) -- all validated.

**What did not work:**

- **Age-inappropriate rationales.** Henderson found that approximately half the Y2 Maths rationales contain KS3/KS4 language. A Y2 measurement cluster rationale references "area, volume and perimeter formulae." A Y2 money cluster rationale references "algebraic manipulation" and "factorisation rules." Kapoor flagged the same for Y5 Science: lens key questions work for Y5 but may not work for Y2 without simplification. Recommendation from Henderson and Kapoor: differentiate lens AI instructions and rationales by key stage.

- **Duplicate rationales across clusters.** Kapoor identified that both Light clusters (CL001 and CL002) have identical rationale text for the Cause and Effect lens, despite one being an investigation cluster and the other an explanation/modelling cluster. Osei confirmed the same pattern across multiple KS3 Biology domains. The rationales appear to have been generated at the topic level, not the cluster level. This contradicts the design claim that "the rationale on each rel explains why the lens fits this specific cluster."

- **Repetitive lens assignments in Biology.** Osei found that CL004-CL008 in KS3 Biology D002 all get Cause and Effect or Systems and System Models as primary. The dominance of two lenses across five clusters suggests either the lens set needs more biology-specific options or the assignment defaults to safe choices.

- **No lesson-level lens guidance.** Adeyemi used 3-4 different lenses across a 5-lesson sequence on the same cluster. Lenses are assigned to clusters (multi-lesson sequences), but different lessons within a cluster benefit from different lenses. A finer assignment granularity would help AI content generation.

**Consensus Thinking Lens rating: 7.5/10 for concept, 5.5/10 for execution (dragged down by age-inappropriate rationales and duplication).**

---

## 3. Critical Data Errors Found

Every teacher found at least one data error. Compiled in severity order:

### Systematic errors

| Error | Teacher | Severity | Detail |
|-------|---------|----------|--------|
| **KS3 Science concept ID misalignment** | Osei | CRITICAL | 6 of 8 KS3 Science Content Vehicles have wrong `delivers_concept_ids`. CV002 (Acids) delivers genetics concepts. CV005 (Particle Model) delivers ecology concepts. CV006 (Ecosystems) delivers respiration concepts. Only CV001 (Cells) and CV004 (Photosynthesis) are correct. IDs appear shifted by one or two domains. Any AI consuming this data would generate nonsensical lesson plans. |
| **KS2 Science CV001 concept IDs** | Kapoor | HIGH | CV001 (Friction Investigation) delivers SC-KS2-C002, C003, C005 -- these are Animals/Plants concepts, not Forces concepts. Should deliver C025 and C027 only. |

### Content errors

| Error | Teacher | Severity | Detail |
|-------|---------|----------|--------|
| **EN-Y4-CV004 wrong book** | Okonkwo | HIGH | *A Great Big Cuddle* by Michael Rosen is listed as a Y4 poetry text. It is a picture book for babies and toddlers (ages 0-5) containing action rhymes and baby songs. Replace with *Quick Let's Get Out of Here* or *Michael Rosen's Big Book of Bad Things*. |
| **Y2 Maths CV001 missing common error** | Henderson | MODERATE | Flexible partitioning errors omitted. Children who can partition 34 into 30+4 often cannot partition as 20+14. This is the direct prerequisite for subtraction with exchange. |
| **Y2 Maths CV006 notation issue** | Henderson | MODERATE | Success criterion wording could cause an AI to generate decimal notation questions (1.50) for Y2, where decimal points are not yet taught. |
| **GE-KS3-CV002 GDP figure** | Adeyemi | MINOR | Japan GDP listed as $5.5 trillion; 2011 actual was $6.16 trillion. |
| **GE-KS3-CV008 US comparator** | Adeyemi | MINOR | "Area smaller than Iowa" -- UK audience should use a UK comparator. |
| **HI-KS3-CV004 no women** | Adeyemi | MODERATE | Key figures list (Churchill, Gandhi, MLK, Mandela) contains no women for a period that includes the suffragette movement and the welfare state. |

### Age-inappropriate Thinking Lens rationales

| Error | Teacher | Severity | Detail |
|-------|---------|----------|--------|
| **KS3/4 rationales on Y2 clusters** | Henderson | MODERATE | ~50% of Y2 Maths lens rationales reference KS3/4 content (area formulae, algebraic manipulation, factorisation). |
| **Duplicate rationales across clusters** | Kapoor, Osei | MODERATE | Multiple domain clusters share identical rationale text despite being different cluster types. |

---

## 4. What's Still Missing (Consensus Gaps)

### Flagged by all 5 teachers

- **Worked examples / step-by-step methods.** Henderson needs worked examples for Maths. Kapoor and Osei need method steps for Science investigations. Okonkwo needs model text excerpts. Adeyemi needs worked source analysis examples. Every subject needs structured, step-by-step instructional content that an AI can follow to model thinking for pupils. Content Vehicles say WHAT to teach but not HOW to walk through it.

### Flagged by 4 teachers

- **Difficulty sub-levels within concepts.** Henderson, Osei, Kapoor, Adeyemi all flag that single concepts span enormous difficulty ranges. Henderson's example: MA-Y2-C005 covers 34+5 (trivial) through 42-17 with exchange (challenging). Adeyemi: GE-KS3-C001 covers Earth's structure through hazard management -- a 5-week unit. No AI can scaffold or differentiate without sub-levels.

- **Incomplete vehicle coverage.** Henderson: 3 of 23 Y2 Maths concepts have no vehicle, including the first topic of the year (counting in steps). Okonkwo: several Y4 English concepts delivered by only one vehicle. Osei: D013 (Forces Y5) has zero vehicles despite being the most investigation-rich Y5 domain. Adeyemi: 8 of 10 Geography vehicles have empty resource arrays.

### Flagged by 3 teachers

- **Safety data inadequate for Science.** Kapoor, Osei, and Henderson (for Science) all flag that safety notes are minimal single sentences. No CLEAPSS references anywhere. Osei provides a detailed example of what a real risk assessment requires (5 columns: hazard, risk, who, control, CLEAPSS ref). CV007 safety notes miss darkened room trip hazards, reflected sunlight, mirror risks, and laser pointer safety.

- **No lesson count or timing estimates.** Henderson: teaching_weight (1-6) does not translate to lesson count; his D002-CL001 took 10 lessons while D001-CL003 took 2. Kapoor: Content Vehicles describe activities with real time constraints but give no time estimates. Adeyemi: different vehicles represent vastly different time investments.

### Flagged by 2 teachers

- **nc_year still missing from KS2 Science (Kapoor).** Every concept is "KS2, Age 7-11." Without year attribution, an AI gives equal weight to Y3 retrieval and Y6 new learning. The data exists in extraction JSONs but is not surfaced.

- **KS2-KS3 transition links still weak (Kapoor, Osei).** Only one Biology prerequisite crosses the KS2-KS3 boundary. Pupils arrive at Y7 with KS2 knowledge of animals, habitats, and evolution that the graph does not model as formal prerequisites.

- **Concept granularity too coarse for Humanities (Adeyemi).** History has 4 concepts for 1000 years. Geography has 7 concepts for a subject that typically has 20+ teachable units. Content Vehicles partially compensate, but the underlying concept model is too coarse for mastery tracking.

- **Missing statutory Geography content (Adeyemi).** Approximately 40% of KS3 Geography is not represented: rivers, coasts, glaciation, weather, economic sectors, energy resources.

### Flagged by 1 teacher

- **Reading content domain codes not mapped (Okonkwo).** KS2 Reading SAT content domains (2a-2h) are not linked to reading concepts, blocking test-preparation generation for reading.
- **Spoken language still disconnected from reading/writing (Okonkwo).**
- **Missing interaction types for Maths (Henderson).** Still no array builder, part-whole model, coin manipulative, clock face, fraction wall.
- **No inter-domain teaching sequence for KS3 Biology (Osei).** Cluster sequencing works within domains but not across them.
- **Biology concept-skill links thin (Osei).** 18 KS3 links are skewed towards Chemistry and Physics. Biology needs 8-10 more (diffusion, photosynthesis, ecosystems, natural selection).

---

## 5. Prioritised Recommendations

### Critical (blocks AI content generation or causes incorrect output)

| # | Recommendation | Teachers | Effort | Impact |
|---|---------------|----------|--------|--------|
| 1 | **Fix KS3 Science Content Vehicle concept IDs.** 6 of 8 vehicles map to wrong concepts. | Osei | Low (JSON fix) | Without this, any AI using KS3 Science vehicles generates nonsense. |
| 2 | **Fix KS2 Science CV001 concept IDs.** Remove Animals/Plants concepts from Friction vehicle. | Kapoor | Low (JSON fix) | Incorrect concept coverage reporting. |
| 3 | **Fix EN-Y4-CV004 text recommendation.** Replace *A Great Big Cuddle* (ages 0-5) with age-appropriate Rosen collection. | Okonkwo | Low (JSON fix) | Wrong text undermines the poetry unit. |
| 4 | **Surface nc_year from extraction data to graph.** Data exists in JSONs, not imported. | Kapoor | Low (pipeline fix) | Without it, no AI can distinguish Y3 retrieval from Y6 new learning. |

### High (significantly improves content generation quality)

| # | Recommendation | Teachers | Effort | Impact |
|---|---------------|----------|--------|--------|
| 5 | **Add worked examples / method steps to Content Vehicles.** 2-3 per vehicle, structured with steps, narration, and predict/pause points. | All 5 | Medium | Transforms generation from "structurally correct" to "actually teachable." Henderson: "the single intervention that would move content readiness from 6/10 to 8/10." |
| 6 | **Add difficulty sub-levels within multi-type concepts.** Specify calculation types (Maths), investigation complexity (Science), sub-topics (Humanities) as ordered difficulty tiers. | Henderson, Osei, Kapoor, Adeyemi | Medium | Enables differentiation and adaptive progression. |
| 7 | **Expand safety notes to hazard-specific format with CLEAPSS references.** Structured array: hazard, risk level, control measure, CLEAPSS ref. | Kapoor, Osei | Medium | UK school science requires CLEAPSS. Without it, the platform cannot claim to support practical science to the expected standard. |
| 8 | **Close Content Vehicle coverage gaps.** Y2 Maths counting/time/position (Henderson), D013 Forces Y5 (Kapoor), empty Geography resource arrays (Adeyemi), concept-to-vehicle audit for English (Okonkwo). | All 5 | Medium | Fills Day 1 gaps and ensures no domain has zero vehicles. |
| 9 | **De-duplicate Thinking Lens rationales.** Write cluster-specific rationales; same-domain clusters should never share identical text. | Kapoor, Osei | Small | Delivers on the design promise of per-cluster reasoning. |
| 10 | **Differentiate lens AI instructions by key stage.** Y1-2 version, Y3-4 version, Y5-6 version, KS3 version. | Henderson, Kapoor | Small | Makes lens instructions usable across the age range. |

### Medium (improves quality and coverage)

| # | Recommendation | Teachers | Effort | Impact |
|---|---------------|----------|--------|--------|
| 11 | **Add reading level / reading mode per suggested text in English.** Even "independent / guided / shared" tags. | Okonkwo | Small | Enables appropriate text selection by AI. |
| 12 | **Map KS2 Reading content domain codes (2a-2h) to concepts.** | Okonkwo | Small | Unlocks reading assessment generation. |
| 13 | **Add lesson count estimates per cluster** (or a formula: base count + weight multiplier). | Henderson, Kapoor | Small | Enables automated scheme-of-work generation. |
| 14 | **Expand Biology concept-skill links.** Add diffusion, photosynthesis, ecosystems, natural selection, respiration. | Osei | Small | Reaches parity with Chemistry and Physics coverage. |
| 15 | **Add text summaries per English vehicle.** Chapter synopses (non-copyrighted) for AI question generation. | Okonkwo | Medium | Enables text-specific comprehension activities. |
| 16 | **Break Humanities concepts into sub-concepts.** Geography from 7 to ~20, History from 4 to ~15. | Adeyemi | High | Essential for mastery tracking. Content Vehicles compensate but do not replace proper concept granularity. |
| 17 | **Add missing statutory Geography content.** Rivers, coasts, glaciation, weather, economic sectors. | Adeyemi | High | Fills ~40% of teaching time currently unrepresented. |
| 18 | **Add sensitivity notes to politically sensitive vehicles.** Boko Haram (GE-KS3-CV004), Israel/Palestine (CV010). The Holocaust vehicle (HI-KS3-CV005) does this well -- others should follow its model. | Adeyemi | Small | Critical for responsible AI content generation on sensitive topics. |

### Low (incremental improvements)

| # | Recommendation | Teachers | Effort | Impact |
|---|---------------|----------|--------|--------|
| 19 | Add concept-driven interaction types (array builder, bar model, clock, coin, fraction wall). | Henderson | Large | Enables maths-specific interactive activities. |
| 20 | Add spoken language cross-domain links to reading/writing. | Okonkwo | Small | Reflects how English is actually taught. |
| 21 | Add KS2-KS3 prerequisite links for Biology. | Osei | Medium | Enables Y7 transition planning. |
| 22 | Add inter-domain sequencing for KS3 Biology. | Osei | Small | Enables full-year scheme-of-work generation. |
| 23 | Add contrasting_with property to History vehicles. | Adeyemi | Small | Enables comparative analysis activities. |
| 24 | Add Tier 2 vocabulary progression per vehicle for English. | Okonkwo | Medium | Supports cross-curricular vocabulary building. |

---

## 6. v4 -> v5 Scorecard

Scoring each item from the v4 group report's "What We All Agree Is Missing" (Section 2):

| v4 Gap | Status | Detail |
|--------|--------|--------|
| **2.1 Subject-specific content vehicles** (5/5 teachers, CRITICAL) | Addressed | Content Vehicles exist for all 5 subjects. Coverage is incomplete (some domains have no vehicle, some concepts undelivered) but the architecture is in place and populated. |
| **2.2 Disciplinary skills disconnected from content** (4/5, CRITICAL) | Partially addressed | 34 concept-level DEVELOPS_SKILL links created (Science). Historical Thinking skills now threaded through History concepts (7 skills, 3 concepts). Biology coverage thin. Spoken Language still disconnected in English. |
| **2.3 No cross-subject links** (4/5, CRITICAL) | Addressed | 18+ curated cross-subject CO_TEACHES relationships: Geography-History (4), English-History (2), Science-Geography (2), Science-Maths (2+). All with specific rationales. |
| **2.4 No difficulty sub-levels** (4/5, HIGH) | Not addressed | No change. Concepts still span wide difficulty ranges without internal grading. |
| **2.5 No assessment scaffolding beyond misconceptions** (4/5, HIGH) | Partially addressed | Content Vehicles add success criteria (4 per vehicle) and assessment guidance (3 questions per vehicle). Still no mark scheme templates, question type specifications, or difficulty calibration. |
| **2.6 CASE/NGSS alignment layer empty** (3/5, HIGH) | Partially addressed | Thinking Lenses implement crosscutting concepts (7 of 10 lenses map to NGSS CCCs). The alignment layer itself (CASE framework relationships) remains empty. Osei: "Lenses are a workaround, not a fix." |
| **2.7 Learner profile year bug** (2/5, MEDIUM) | Addressed | Fixed. Y2 now gets Y2 profile data. Henderson confirmed correct number range, sentence length, FK grade, and session length. |

**v4 Tier 1 Critical recommendations scorecard:**

| v4 Rec | Status | Score |
|--------|--------|-------|
| 1. Add content vehicles | Addressed (coverage incomplete) | Addressed |
| 2. Integrate disciplinary skills | Partially done (34 links, Biology thin) | Partially |
| 3. Add cross-subject relationships | Done (18+ curated links) | Addressed |
| 4. Fix learner profile year mapping | Done | Addressed |

**v4 Tier 2 High recommendations scorecard:**

| v4 Rec | Status | Score |
|--------|--------|-------|
| 5. Add difficulty sub-levels | Not done | Not addressed |
| 6. Add assessment scaffolding | Partially (CV success criteria) | Partially |
| 7. Add nc_year to KS2 concepts | Not done (data exists, not surfaced) | Not addressed |
| 8. Break History into sub-concepts | Not done (vehicles compensate) | Not addressed |
| 9. Add missing interaction types | Not done | Not addressed |

**v4 Tier 3 Medium recommendations scorecard:**

| v4 Rec | Status | Score |
|--------|--------|-------|
| 10. Add NGSS Crosscutting Concepts as tags | Done (Thinking Lenses) | Addressed |
| 11. Map KS2 Reading content domain codes | Not done | Not addressed |
| 12. Add CPA as structured data | Done (via Content Vehicles) | Addressed |
| 13. Add genre/text-type taxonomy for English | Done (via Content Vehicles) | Addressed |
| 14. Add missing statutory Geography content | Not done | Not addressed |
| 15. Populate CASE alignment layer | Not done (Lenses are partial workaround) | Not addressed |

**Summary: 7 of 15 v4 recommendations addressed or substantially addressed. 3 partially addressed. 5 not addressed.** The development team correctly prioritised the Tier 1 critical items (content vehicles, cross-subject links, year bug) and the crosscutting concept request. The unaddressed items are predominantly Tier 2 and Tier 3 work that requires deeper extraction or structural changes (difficulty sub-levels, concept granularity, missing curriculum content).

---

## 7. What Would Move Us to 9/10

Each teacher was asked what it would take to rate the graph 9/10 for content generation readiness. Their answers converge on three themes:

### Theme 1: Add worked examples and method steps (all 5 teachers)

Henderson: "Hire a Y2 maths specialist for two weeks to write worked examples and difficulty progressions for CV001-CV008. That single intervention would move content readiness from 6/10 to 8/10." Kapoor: add numbered method steps to Science vehicles. Osei: the same. Okonkwo: add model text excerpts or chapter synopses. Adeyemi: add worked source analysis examples for History.

This is the highest-leverage investment. Content Vehicles say WHAT to teach. Worked examples show HOW to teach it. The architecture supports this -- it is a data enrichment task, not a redesign.

### Theme 2: Fix data errors and close coverage gaps (all 5 teachers)

The KS3 Science concept ID errors (Osei) must be fixed before the graph can be used by any AI system for KS3 Science. The KS2 Science CV001 error (Kapoor) and the English CV004 book error (Okonkwo) are smaller but still trust-breaking. Vehicle coverage gaps (Henderson: 3 concepts with no vehicle; Adeyemi: empty resource arrays) mean the graph cannot yet support a full year of planning in any subject.

Fix the errors first. Then close the gaps.

### Theme 3: Add difficulty sub-levels for differentiation (4/5 teachers)

Henderson: "The gap from 6 to 10 is the difference between 'structurally correct' and 'actually usable without teacher modification.'" The graph currently provides a single path through each concept. Real classrooms have children working at three levels simultaneously. Without within-concept difficulty grading, no AI can differentiate, scaffold, or adapt.

### The 9/10 formula

If the team delivered these three things -- worked examples in every Content Vehicle, all data errors fixed with full coverage, and difficulty sub-levels within multi-type concepts -- the consensus rating would move from 6.6/10 to approximately 8.5/10. The final half-point to 9/10 requires the subject-specific gaps each teacher identified: CLEAPSS references for Science, reading content domain codes for English, concept granularity for Humanities, and interaction types for Maths.

The nature of the remaining work is not architectural -- the architecture is sound. It is content quality work that requires subject specialist input. As Henderson put it: "Get a teacher to do it."

---

*"The architecture is right. The content needs filling in."* -- Henderson

*"This graph is ready for pilot use. I would use it to plan my Year 4 English teaching."* -- Okonkwo

*"I would now trust an AI to generate a structured science investigation from this graph."* -- Kapoor

*"Fix the concept IDs. Everything else is incremental."* -- Osei

*"From a curriculum map that could generate diagnostic questions to a curriculum framework that can support lesson planning. Not yet a content generation engine -- but significantly closer."* -- Adeyemi
