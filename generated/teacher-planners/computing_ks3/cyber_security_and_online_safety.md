# Computing | Teacher Planner: Cyber Security and Online Safety
*[TS-CO-KS3-006]*

**Subject:** Computing | **Key Stage:** KS3 | **Year group:** Y7, Y8, Y9
**Statutory reference:** understand a range of ways to use technology safely, respectfully, responsibly and securely, including protecting their online identity and privacy | **Source document:** Computing (KS3/KS4) - National Curriculum Programme of Study
**Estimated duration:** 6 lessons | **Status:** Mandatory

**Planner coverage:** 8/10 expected capabilities surfaced

**Available now:** Curriculum anchor, Concept model, Differentiation data, Thinking lens, Lesson structure, Cross-curricular links, Prior knowledge links, Learner scaffolding
**Still thin/missing:** Vocabulary definitions, Success criteria

---

## Concepts

This study delivers **1 primary concept** and **0 secondary concepts**.

### Primary concept: Ethics, Privacy and the Social Impact of Computing (CO-KS34-C004)

**Type:** Knowledge | **Teaching weight:** 3/6

Computing technologies are transforming society in profound ways, creating new opportunities and new risks. Algorithmic decision-making - using programs to make or assist decisions about credit, employment, criminal sentencing, medical diagnosis - raises questions about fairness, transparency and accountability when algorithms encode or amplify biases. Mass surveillance enabled by digital data collection challenges privacy and civil liberties. Automation threatens some forms of employment while creating others. Artificial intelligence raises questions about responsibility, creativity and the nature of intelligence. At KS3 and KS4, pupils develop the conceptual tools to engage with these ethical and social dimensions of computing as informed citizens.

**Teaching guidance:** Use real-world case studies to anchor abstract ethical discussions: algorithmic bias in hiring tools, facial recognition errors, data breaches. Teach pupils to identify stakeholders in technology systems and whose interests may be served or harmed. Develop vocabulary for ethical analysis: fairness, accountability, transparency, consent, privacy. Explore both the benefits and the risks of AI, social media and surveillance technologies. Connect to legal frameworks: GDPR, computer misuse legislation, intellectual property. Encourage pupils to evaluate technology critically rather than accepting or rejecting it wholesale.

**Key vocabulary:** ethics, privacy, surveillance, algorithm, bias, fairness, transparency, accountability, artificial intelligence, automation, data, consent, right, responsibility, impact

**Common misconceptions:** Pupils may assume that algorithms are neutral because they are mathematical. The data used to train machine learning models reflects historical human decisions and can encode and amplify historical biases. The idea that privacy is only a concern for people with 'something to hide' is common but misguided; developing nuanced understanding of privacy as a social good prevents this. Technology is neither inherently good nor bad; its impacts depend on how it is designed, deployed and governed.

#### Differentiation

| Level | What success looks like | Example task | Common errors |
|-------|------------------------|-------------|---------------|
| **Emerging** | Knows that personal information should be kept private online and recognises obvious online risks (e.g., sharing passwords), but does not understand the broader social or ethical dimensions of computing. | Give two reasons why you should not share your password with friends. | Saying only 'it is dangerous' without explaining the specific risks; Not recognising that even trusted friends create a chain of vulnerability |
| **Developing** | Understands key concepts such as data privacy, digital footprint and intellectual property, and can identify ethical issues in straightforward scenarios involving technology. | Explain what a 'digital footprint' is and why it matters for your future. | Thinking that deleting a post removes it completely, not understanding that data may have been cached, screenshotted or archived; Not recognising the commercial use of digital footprint data by advertisers |
| **Secure** | Analyses the social impact of computing technologies including algorithmic bias, surveillance and automation, using structured ethical frameworks to evaluate benefits and harms to different stakeholders. | An AI system is used to decide which job applicants are invited to interview. Explain how this could be biased and identify who might be harmed. | Assuming that because the system is automated it must be objective and fair; Identifying that bias exists without explaining the mechanism by which it enters the system (through biased training data) |
| **Mastery** | Evaluates complex ethical dilemmas in computing with nuance, considers multiple perspectives and competing values, proposes governance frameworks, and connects technical decisions to their societal consequences. | Should governments have the ability to access encrypted messages to prevent terrorism? Argue both sides and state your position with justification. | Presenting only one side of the argument without genuinely engaging with the opposing view; Not understanding the technical reality that a backdoor for governments is also a backdoor for hackers |

> **Model response (Emerging):** *1) If a friend knows your password, they could access your accounts and read private messages, change your settings or post things pretending to be you. 2) Even if your friend is trustworthy, they might accidentally reveal the password to someone else, or their own device could be compromised, exposing your password to hackers.*

> **Model response (Developing):** *A digital footprint is the trail of data you leave behind when using the internet: social media posts, comments, photos, search history, website visits, and online purchases. It matters because this data is often permanent and searchable. Employers routinely search candidates' social media before hiring — a post made at age 14 could affect a job application at age 25. Universities, landlords and even potential partners may search your name. Your digital footprint also determines what advertisements and content you are shown, as companies build profiles from your data to target you commercially.*

> **Model response (Secure):** *The AI system is trained on historical hiring data — data about which candidates were previously selected by human recruiters. If those recruiters had biases (conscious or unconscious) — for example, preferring candidates from certain universities, with certain names, or of certain genders — the AI will learn to replicate those biases, encoding them into its algorithms. The system might systematically disadvantage: women (if the company historically hired mainly men); ethnic minorities (if names associated with certain backgrounds were previously filtered out); people with non-traditional career paths (if the training data only includes conventional backgrounds). The harm is significant: qualified candidates are rejected not because they are unsuitable but because the AI has learned to discriminate. The bias is harder to detect than human bias because the algorithm appears objective — people trust 'the computer said no' more than 'the manager said no'. Stakeholders harmed: rejected candidates (denied opportunities), the company (misses diverse talent), and society (systemic inequality is reinforced and automated at scale).*

> **Model response (Mastery):** *For government access: terrorism, child exploitation and organised crime use encrypted messaging to coordinate harmful activities. If law enforcement cannot access these communications even with a court order, serious crimes may go undetected and lives may be lost. Democratic societies already permit surveillance warrants for phone calls and letters — encrypted digital communication should not be exempt from the rule of law. Against government access: creating a 'backdoor' in encryption that the government can access fundamentally weakens the encryption for everyone. Security researchers have consistently demonstrated that any backdoor intended for government use can be discovered and exploited by criminals and hostile states. The same weakened encryption would compromise the communications of journalists protecting sources, political dissidents in authoritarian regimes, businesses protecting trade secrets, and ordinary citizens' private conversations. Once a backdoor exists, there is no way to guarantee it is used only by authorised parties. My position: strong encryption without backdoors should be maintained. The security of billions of private communications should not be compromised because a small number of criminals use the same technology. Alternative investigative methods (metadata analysis, undercover operations, device seizure with warrants) can address criminal use without weakening the infrastructure that protects everyone. The history of surveillance powers shows consistent mission creep — powers granted for terrorism are routinely expanded to lesser offences.*

---

## Thinking lens: Perspective and Interpretation (primary)

**Key question:** Whose perspective is this, what shapes it, and what might be missing?

**Why this lens fits:** Analysing the ethical implications of algorithmic decision-making, surveillance and AI requires pupils to evaluate the same technology from the perspectives of different affected groups — individuals, communities, corporations and governments — making perspective-taking the central intellectual demand.

**Question stems for KS3:**
- What contextual factors shaped this perspective?
- How does the author's position affect the reliability of this account?
- Whose perspective is missing from this record, and why does that matter?
- How have interpretations of this event changed over time, and what drove those changes?

**Secondary lens:** Cause and Effect — The social impact strand asks pupils to trace the second- and third-order causal consequences of technological choices — how widespread surveillance causes chilling effects on free expression, or how algorithmic bias perpetuates systemic discrimination.

---

## Session structure: Discussion and Debate

### Discussion and Debate
A structured sequence for exploring contested issues or multiple perspectives. Begins with a stimulus that raises a question or dilemma, builds knowledge through research, develops arguments through structured discussion techniques, captures thinking in writing, and reflects on how views may have changed.

`stimulus` → `research` → `structured_discussion` → `writing` → `reflection`

**Assessment:** Balanced written argument or persuasive piece demonstrating understanding of multiple perspectives, supported by evidence, with a reasoned personal conclusion.

**Teacher note:** Use the DISCUSSION AND DEBATE template: present a substantive question or ethical dilemma. Expect pupils to research different perspectives and prepare evidence-based arguments. Facilitate structured discussion using protocols such as Harkness or four corners. Guide pupils to produce a written response that acknowledges multiple viewpoints and justifies their own position.

**KS3 question stems:**
- What are the strongest arguments on each side of this issue?
- What evidence supports this perspective, and how reliable is it?
- How would you respond to the main counter-argument?
- How has the discussion changed or strengthened your view?

---

## Computing focus

**Computational concepts:** digital literacy, networking
**Abstraction level:** Visual
**Themes:** cyber security, online safety, data protection, privacy

---

## Why this study matters

KS3 online safety escalates from KS2's personal safety focus to understanding the technical and social dimensions of cyber security. Pupils learn how attacks work (phishing, social engineering, malware, SQL injection at a conceptual level), why passwords are important (hashing, brute force), and how organisations protect data (encryption, firewalls, access control). Understanding the attack surface makes pupils more effective at protecting themselves and prepares for GCSE content on ethical hacking and network security.

---

## Pitfalls to avoid

1. Scare-based teaching without empowerment -- teach what TO do, not just what to fear
2. Only covering passwords -- cyber security encompasses social engineering, phishing, network security and data protection
3. Not connecting to GDPR and data protection law -- KS3 pupils need to understand their data rights

---

## Cross-curricular opportunities

| Link | Subject | Connection | Strength |
|------|---------|------------|----------|
| Human Rights: What Are They and Why Do They Matter? | General | Human rights, digital rights, privacy and surveillance | Moderate |

---

## Computational thinking skills (KS3)

These disciplinary skills should be woven through teaching, not taught in isolation:

- **Abstraction (KS1)** — Focus on the most important features of a problem or task while ignoring unnecessary detail; represent real-world actions as simple step-by-step instructions that capture the essential logic without irrelevant specifics.
- **Abstraction (KS2)** — Design programs and digital solutions by identifying the key variables, inputs and outputs relevant to a problem while deliberately ignoring peripheral details; use procedures and functions as abstractions that hide implementation complexity; apply abstraction when planning programs using pseudocode or flowcharts.
- **Abstraction (KS3)** — Design and evaluate computational abstractions that model the state and behaviour of real-world problems and physical systems; select appropriate levels of abstraction for a given problem context; use abstract data types, classes and interfaces to hide implementation detail; understand the layered abstractions present in computing systems from hardware to application.
- **Decomposition (KS1)** — Break a familiar task or problem into a sequence of smaller, ordered steps; understand that a complex instruction can be split into simpler sub-instructions that together achieve the same goal; apply this thinking when giving instructions to a programmable toy or creating a simple program.
- **Decomposition (KS2)** — Decompose a complex programming problem or digital project into distinct, manageable sub-problems that can be developed and tested independently; plan program structure using top-down design before coding; use procedures and functions as the coded expression of decomposed sub-problems.
- **Decomposition (KS3)** — Design and develop modular programs that use procedures and functions to decompose complex problems; apply top-down and bottom-up design strategies; decompose data requirements as well as procedural logic, selecting appropriate data structures for each sub-problem; evaluate how modular decomposition improves code readability, maintainability and reuse.

---

## Vocabulary word mat

| Term | Meaning |
|------|---------|
| cyber security | |
| phishing | |
| social engineering | |
| malware | |
| encryption | |
| firewall | |
| authentication | |
| two-factor authentication | |
| GDPR | |
| data protection | |
| hashing | |
| brute force | |
| accountability | *(from concept key vocabulary)* |
| algorithm | *(from concept key vocabulary)* |
| artificial intelligence | *(from concept key vocabulary)* |
| automation | *(from concept key vocabulary)* |
| bias | *(from concept key vocabulary)* |
| consent | *(from concept key vocabulary)* |
| data | *(from concept key vocabulary)* |
| ethics | *(from concept key vocabulary)* |
| fairness | *(from concept key vocabulary)* |
| impact | *(from concept key vocabulary)* |

## Prior knowledge (retrieval plan)

Pupils should already know the following from earlier units:

| Prior knowledge needed | For concept | Description |
|----------------------|-------------|-------------|
| Online Safety and Digital Citizenship | Ethics, Privacy and the Social Impact of Computing | Online safety encompasses the knowledge, skills and behaviours needed to participate in digital e... |

---

## Scaffolding and inclusion (Y7)

| Guideline | Detail |
|-----------|--------|
| Reading level | Secondary Transition Reader (Lexile 700–950) |
| Text-to-speech | Available |
| Max sentence length | 30 words |
| Vocabulary | Secondary curriculum vocabulary including discipline-specific terms. Etymology and morphology appropriate (e.g., prefixes, roots). Formal academic register expected. |
| Scaffolding level | Light |
| Hint tiers | 4 tiers |
| Session length | 25–40 minutes |
| Worked examples | Required — Text-based. Reference solutions available after independent attempt. |
| Feedback tone | Academic Peer |
| Normalize struggle | Yes |
| Example correct feedback | *Correct — and the implication is worth noting: if this is true, then [connected consequence] should also hold. Does it?* |
| Example error feedback | *That reasoning has a gap: you assumed [X], but the evidence points the other way because [Y]. Revise your argument in light of that.* |

---

## Knowledge organiser

**Key terms:**
- cyber security
- phishing
- social engineering
- malware
- encryption
- firewall
- authentication
- two-factor authentication
- GDPR
- data protection
- hashing
- brute force

**Core facts (expected standard):**
- **Ethics, Privacy and the Social Impact of Computing**: Analyses the social impact of computing technologies including algorithmic bias, surveillance and automation, using structured ethical frameworks to evaluate benefits and harms to different stakeholders.

---

## Graph context

**Node type:** `ComputingTopicSuggestion` | **Study ID:** `TS-CO-KS3-006`

**Concept IDs:**
- `CO-KS34-C004`: Ethics, Privacy and the Social Impact of Computing (primary)

**Cypher query:**
```cypher
MATCH (ts:ComputingTopicSuggestion {suggestion_id: 'TS-CO-KS3-006'})
  -[:DELIVERS_VIA]->(c:Concept)
  -[:HAS_DIFFICULTY_LEVEL]->(dl)
RETURN c.name, dl.label, dl.description
```

---

*Generated from the UK Curriculum Knowledge Graph — zero LLM generation.*
