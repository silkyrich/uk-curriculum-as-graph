# Lesson Plan: Ask Scientific Questions, Make Predictions and Design Investigations

| Field | Value |
|---|---|
| **Cluster ID** | `SC-KS3-D001-CL002` |
| **Cluster type** | Practice |
| **Subject** | Science |
| **Year group** | Year 9 |
| **Key stage** | KS3 (KS3/4 transition) |
| **Estimated teaching time** | 5 lessons (~2 weeks at a double and three singles, or equivalent) |
| **Domain** | Working Scientifically (`SC-KS3-D001`) |
| **Concepts covered** | Scientific questioning (C007), Scientific prediction (C008), Variables in experiments (C009), Experimental design (C010) |

> **Note on cluster positioning:** Cluster SC-KS3-D001-CL002 was first introduced in Y7. This Y9 lesson plan treats it as a formalisation and consolidation sequence for pupils approaching the KS3/KS4 boundary, not a first encounter. The cluster content is the same; the depth of treatment, the demand of the tasks, and the register of feedback are calibrated to Y9.

---

## 1. At a Glance

This cluster addresses the planning stage of scientific enquiry: asking testable questions, making justified predictions, identifying and controlling variables, and selecting appropriate enquiry types. In Y7, pupils were introduced to these skills in accessible contexts with moderate scaffolding. By Y9, pupils must be able to apply these skills with precision, justify their choices explicitly, and identify weaknesses in their own and others' designs — precisely the analytical demands that appear in KS4 required practicals, GCSE method questions, and, for some, A-level investigation write-ups.

Y9 is the critical moment for three reasons. First, pupils have encountered the five enquiry types and variable identification across multiple topic areas (biology, chemistry, physics) since Y7, giving them the breadth of prior experience needed to generalise and discriminate. Second, they are close enough to GCSE that the language, register, and analytical expectations of the exam can be introduced authentically without feeling premature. Third, misconceptions about investigation design — particularly the conflation of enquiry types, the treatment of control variables as optional, and the confusion of prediction with hypothesis — are well-entrenched by Y9 and require direct, explicit challenge rather than incidental correction.

The aim of this lesson sequence is not to re-teach what a fair test is. It is to make implicit procedural knowledge explicit, to push pupils toward evaluative and analytical thinking about method quality, and to introduce the vocabulary and reasoning structures they will need to access 6-mark practical method questions at GCSE.

---

## 2. Curriculum Context

### The KS3 Working Scientifically progression

**KS2 (Years 3–6) built:**
- Fair testing as the default enquiry model (change one thing, keep everything else the same, measure the result)
- Informal use of prediction as a sentence starter ("I think... because...")
- Identification of what to change and what to measure in prescribed contexts
- Basic variable control as a procedural rule, not as principled reasoning

**Y7/8 developed:**
- Introduction of the five enquiry types as a formal framework: observational/longitudinal studies, pattern-seeking investigations, identifying and classifying, comparative fair testing, and research using secondary sources
- The independent/dependent/control variable triad as named concepts applied across biology, chemistry, and physics contexts
- Prediction as requiring justification from prior knowledge, not just a guess
- The idea that questions must be testable — that is, answerable through observation or measurement

**Y9 extends and formalises:**
- Discrimination between enquiry types at the level of justification (not just naming): pupils should be able to explain why pattern-seeking is more appropriate than a fair test for investigating biodiversity, or why a longitudinal study is required to examine ecosystem change
- Critique of experimental designs: identifying not just what variables must be controlled but what will happen if they are not — reasoning about confounding variables and the validity of conclusions
- Prediction as a formal scientific claim: linking to mechanistic understanding, not just pattern extrapolation
- Operational definitions of variables: specifying exactly how an independent or dependent variable will be measured, including units, range, and interval
- Quantitative reasoning about sample size and its effect on reliability — connecting to statistical thinking developed in SC-KS3-D001-CL007

**Feeds into KS4:**
- Required practicals at GCSE (all exam boards) require pupils to plan, carry out, and evaluate investigations. Method questions at GCSE typically ask pupils to (a) identify variables, (b) explain how to control them, (c) justify their choice of equipment, and (d) evaluate the reliability and validity of data.
- CPAC (Practical Competencies for A-level) and GCSE-level ISA/internal assessment components require written planning that mirrors the skills consolidated in this cluster.
- A-level biology, chemistry, and physics all include hypothesis-driven experimental design as a core component of the examined skills paper.

This cluster is the last major teaching of enquiry design before GCSE practical demands take over. The transition from rule-following ("change one variable") to principled reasoning ("here is why controlling [x] matters for the validity of the conclusion") must happen here.

---

## 3. Prerequisite Check

### What pupils must already bring

By Y9, pupils should be able to:

- Name and distinguish the five enquiry types
- Identify independent, dependent, and control variables in a described experiment
- Construct a basic prediction using an "I predict... because..." structure
- Distinguish a testable scientific question from a non-scientific question
- Describe what a fair test is and why it is important

### Diagnostic questions — use in Lesson 1, first 10 minutes

These are not formative quizzes but purposeful pre-exposure problems. Collect written responses; do not mark publicly. Use the responses to calibrate your groupings and to identify which misconceptions need priority treatment in Lesson 2.

**Question 1 (enquiry type discrimination):**
A researcher wants to know whether there is a relationship between the distance from a road and the number of lichen species found on trees. Which enquiry type is most appropriate: a fair test, or a pattern-seeking investigation? Justify your choice in two sentences.

*What to look for:* Pupils who say "fair test" are over-applying the default enquiry model from KS2. The correct answer is pattern-seeking, because the researcher cannot manipulate distance — they can only observe it. Any pupil writing "you would change the distance" has a fundamental misunderstanding of what manipulable variables are.

**Question 2 (variable identification and prediction):**
A student investigates how the concentration of hydrochloric acid affects the rate of reaction with marble chips. (a) What is the independent variable? (b) What is the dependent variable? (c) Write a prediction for this investigation with a justification.

*What to look for:* Common Y9 errors — listing "temperature" or "surface area" as the independent variable (confusing control and independent); writing a prediction with no causal mechanism ("it will react faster because of higher concentration" is acceptable but incomplete — the stronger answer references collision frequency and the particulate model); confusing "the reaction will be faster" (prediction) with "higher concentration means more particles in the same volume" (the mechanism, which should appear as the 'because').

**Question 3 (hypothesis vs prediction):**
What is the difference between a hypothesis and a prediction? Write one sentence for each.

*What to look for:* Most Y9 pupils conflate these. A hypothesis is a proposed explanatory mechanism (causal claim about why something happens); a prediction is a specific, testable statement about what will be observed. Many pupils write the reverse or treat them as synonyms. This misconception is worth approximately 2 marks at GCSE (command word: "suggest a hypothesis").

**Question 4 (critique of design):**
A student investigates whether exercise increases heart rate. They measure their resting heart rate, run for 5 minutes, and immediately measure their heart rate again. They repeat this three times on the same day. Identify two limitations of this method.

*What to look for:* Strong Y9 responses identify specific confounding variables (fatigue accumulating between trials; heart rate not returning to true resting baseline; not controlling exercise intensity between trials; single subject means no population-level inference). Weak responses give vague answers ("they should do it more times") without identifying the specific validity or reliability problem.

---

## 4. Learning Objectives

By the end of this cluster sequence, pupils will be able to:

1. **Analyse** a scientific scenario and select the most appropriate enquiry type, providing a justified reason that references the nature of the variables and what kind of conclusion the evidence permits. (Bloom's: Analysis)

2. **Construct** a well-formed scientific prediction that includes an expected outcome, a mechanistic justification drawn from prior scientific knowledge, and an explicit reference to the independent and dependent variables. (Bloom's: Synthesis)

3. **Evaluate** an experimental design by identifying specific threats to validity (confounding variables) and reliability (repeatability, sample size), and proposing targeted improvements with reasoned justifications. (Bloom's: Evaluation)

4. **Distinguish** between prediction and hypothesis, independent and dependent variables, and validity and reliability — demonstrating that these distinctions are not merely terminological but affect the quality of conclusions that can be drawn. (Bloom's: Analysis)

5. **Design** an original investigation — from question through to risk assessment — that includes an operational definition of the dependent variable, explicit justification of control variables, and a reasoned choice of measurement range and interval. (Bloom's: Synthesis/Creation)

---

## 5. Key Vocabulary

Scientific vocabulary for this cluster at Y9 level. Definitions should be precise and distinguish near-synonyms explicitly.

| Term | Definition | Common confusion |
|---|---|---|
| **Scientific question** | A question that can be answered by collecting evidence through observation or measurement | Confused with any "about science" question — philosophical questions ("Is science always right?") are not scientific questions in this sense |
| **Hypothesis** | A proposed explanation for an observed phenomenon, typically expressed as a causal mechanism; a claim about why something happens | Confused with prediction — a hypothesis explains; a prediction specifies what will be observed |
| **Prediction** | A specific, testable statement about the expected outcome of an investigation, justified by prior knowledge or a hypothesis | Treated as a guess; confused with hypothesis (see above) |
| **Independent variable (IV)** | The variable that is deliberately changed or selected by the investigator to test its effect | Confused with the dependent variable; confused with controlled variables that the student changes in setup but not as the investigated factor |
| **Dependent variable (DV)** | The variable that is measured in response to changes in the independent variable | Confused with the independent variable; confused with "what changes" rather than "what is measured" |
| **Control variable** | A variable that is kept constant to ensure that any observed change in the DV is caused by the IV and not by some other factor | Treated as optional; confused with the "control group" (a different concept); treated as "the thing you don't change" rather than "the thing you keep constant to protect validity" |
| **Confounding variable** | A variable that the investigator did not control, which may have caused or contributed to the observed change in the DV | New to many Y9 pupils; often treated vaguely as "sources of error" rather than named specifically |
| **Validity** | The extent to which an investigation measures what it claims to measure; an investigation is valid if the DV truly reflects the effect of the IV and nothing else | Confused with reliability; a precise answer is valid if the measurement matches the true value — a collection of precise measurements can be reliable but invalid if the method introduces systematic bias |
| **Reliability** | The extent to which results can be reproduced — by the same person repeating the investigation (repeatability) or by different people using the same method (reproducibility) | Confused with validity; high reliability does not guarantee validity |
| **Operational definition** | A precise, quantitative specification of how a variable will be measured: units, instrument, measurement range, and method of recording | Rarely used at KS2/3 but required for KS4 planning questions; "measure temperature" is not an operational definition; "measure temperature in °C using a calibrated thermometer to the nearest 0.5°C at one-minute intervals" is |
| **Enquiry type** | One of five distinct approaches to scientific investigation: observational/longitudinal, pattern-seeking, identifying and classifying, comparative fair testing, research using secondary sources | Conflated — most pupils use "fair test" as the default term for any investigation type |
| **Sample size** | The number of individual subjects, specimens, or repeated trials included in an investigation; larger samples increase reliability and allow statistical inference | Confused with number of variables; treated as "how many times you repeat" (which is trials/replicates) rather than the number of independent subjects or specimens |
| **Causal relationship** | A relationship where changing one variable causes a change in another; distinct from correlation, which indicates association without necessarily implying cause | Pupils routinely infer causation from correlation, particularly in pattern-seeking investigations |

---

## 6. Common Misconceptions

### Procedural misconceptions

**M1: All scientific investigations are fair tests.**
This is the dominant misconception entering Y9, and it is the direct legacy of KS2 teaching. The five enquiry types have been taught since Y7, but pattern-seeking and observational studies are significantly less practised and therefore less secure. Y9 pupils given any investigation scenario will default to "you change [x] and measure [y]" framing even when the scenario describes a survey, a classification task, or a longitudinal study. This is not a terminological error — it reflects a genuine failure to understand that not all scientific variables can be manipulated.

*Treatment:* Use scenario discrimination tasks explicitly (see Lesson 2). The key diagnostic question is: "Can the investigator actually change this variable, or can they only observe different levels of it?" If the answer is the latter, a fair test is not appropriate.

**M2: You only need one control variable.**
Y9 pupils frequently identify "temperature" as the sole control variable in enzyme experiments, or "light" in plant growth experiments, and stop there. This reflects the way fair tests are often introduced — with a single worked example where one obvious variable is highlighted. Pupils do not spontaneously list the full set of confounding variables because they have not been taught to generate them systematically.

*Treatment:* Teach a systematic method for generating control variables: identify the DV, ask "what else could cause this to change?", then list all realistic candidates. For each one: "Can we control it? How?" Introduce the idea of residual confounding — variables you cannot control, which must be acknowledged as limitations.

**M3: Repeating an experiment three times is always sufficient.**
Y9 pupils cite "repeat three times and take an average" as a mantra without understanding what it achieves. Three repeats reduces the influence of random error within a set of measurements; it says nothing about whether the sample of subjects is representative or whether the range of the IV has been adequately sampled.

*Treatment:* Distinguish between replicates (repetitions of the same measurement) and sample size (number of independent subjects or specimens). Three replicates of one measurement does not increase the validity of a conclusion about a population.

### Conceptual misconceptions

**M4: Prediction and hypothesis are the same thing.**
This is extremely common at Y9. Pupils write "my hypothesis is that the faster the enzyme will work" (a prediction, not an explanatory mechanism) or "my prediction is that enzymes are denatured at high temperatures because the active site changes shape" (a hypothesis, not a prediction). The distinction matters for GCSE command words: "suggest a hypothesis" requires a mechanistic explanation; "make a prediction" requires a specific expected outcome.

*Treatment:* Use paired examples. Present the same investigation with both a strong prediction and a strong hypothesis. Ask pupils to identify which is which and explain the difference. The key question: "Does this statement tell you what you expect to observe, or does it tell you why?"

**M5: Correlation implies causation in pattern-seeking investigations.**
When pupils conduct or interpret pattern-seeking investigations (e.g., relationship between shoe size and height; relationship between distance from a road and plant diversity), they routinely conclude "this proves that [x] causes [y]". This is not just a scientific literacy error — it reflects a genuine misunderstanding of what different enquiry types can and cannot establish.

*Treatment:* Explicitly address what each enquiry type allows you to conclude. Pattern-seeking shows correlation and can generate hypotheses; only a controlled experiment (with random assignment or equivalent controls) can establish cause. Use a concrete example: does increased shoe size cause increased height, or are both caused by age and developmental stage?

**M6: A well-designed experiment gives you "the right answer".**
Y9 pupils often expect a well-designed experiment to confirm the prediction. They are uncomfortable with results that do not support a prediction, and will often question their method rather than question the hypothesis — or alternatively, declare the experiment "wrong" rather than use it as evidence against their prediction. This reflects a view of science as confirmatory rather than falsificationist.

*Treatment:* Address this directly in the productive failure lesson (Lesson 3). Design the task so that results are likely to be unexpected or ambiguous. The metacognitive debrief must address: what does it mean if your results do not support your prediction?

**M7: The independent variable is "what you measure".**
Despite the mnemonic "I change the Independent, I measure the Dependent", roughly a quarter of Y9 pupils still identify variables incorrectly when the scenario is slightly unfamiliar. The confusion increases when the IV is a qualitative or categorical variable (e.g., "type of insulation material") rather than a quantitative one.

*Treatment:* Use a wide range of surface contexts for variable identification practice, including categorical IVs, multi-level IVs (more than two values), and cases where the DV is difficult to measure directly (requiring a proxy measure).

---

## 7. Lesson Sequence

### Pedagogical framework for Y9

The Y9 pedagogy profile requires a session sequence of:
`challenge_problem → independent_practice → model_answer_comparison → retrieval_practice → metacognitive_reflection`

Worked examples are not shown by default — they are provided as reference after independent attempt. Scaffolding is minimal. Productive failure is expected across all lessons. Challenge problems can span more than one session (multi-day investigation design tasks are appropriate). Spacing intervals for retrieval: 7–35 days.

Desirable difficulties in use across this sequence: **spacing** (retrieval from Y7/8 content in every session), **interleaving** (biology/chemistry/physics contexts mixed), **varied_practice** (same underlying skill across different surface scenarios), **generation_effect** (recall before reveal), **testing_effect** (retrieval-only for mastered concepts).

---

### Lesson 1: Diagnosing and destabilising — what do we actually know about enquiry? (~50 minutes)

**Session sequence:** challenge_problem → independent_practice → model_answer_comparison → metacognitive_reflection

**Learning focus:** Surface prior knowledge, reveal misconceptions, introduce the Y9-level task demand.

**Phase 1 — Retrieval warm-up (7 minutes, `testing_effect`)**
Before any new content, ask pupils to retrieve from memory — no notes. Interleave across Y7/8 science topics.

Present three questions rapidly:
1. Name the five types of scientific enquiry. (Retrieval of framework from Y7.)
2. In an investigation into the effect of light intensity on the rate of photosynthesis, identify the independent variable, the dependent variable, and two control variables. (Variable identification, biology context.)
3. What is the difference between a result being reliable and a result being valid? (Conceptual distinction — likely partially secure but imprecise at Y9.)

Do not mark or discuss yet. Collect written responses. These feed directly into Phase 2.

**Phase 2 — Challenge problem: the flawed investigation (15 minutes, `challenge_problem`, `generation_effect`)**
Present the following scenario in writing. No verbal framing. No hints at this stage.

> *A student is investigating whether eating breakfast improves performance on a maths test. They recruit 30 Year 9 students. On Monday, students are asked to eat breakfast before coming to school. On Wednesday, the same students are asked to skip breakfast. Both groups take a 10-question maths test at 9am on each day. The student concludes: "Eating breakfast improves maths performance because the breakfast group scored higher."*

Task (written, individual, `text_input_paragraph`): Evaluate this investigation. In your response, address: the enquiry type used, whether this is an appropriate design, what variables may have affected the results, and whether the conclusion is justified.

This is a multi-part evaluative task. At Y9, this should be attempted without support. The scenario has several deliberate flaws: no control group, order effects (Monday vs Wednesday), practice effect on the test, fatigue differences, selection bias (same students as own control). The conclusion conflates correlation with causation.

**Phase 3 — Model answer comparison and discussion (15 minutes, `model_answer_comparison`)**
After independent attempt, present a model answer for comparison. The model answer should be at mark-scheme quality: it names the confounding variables specifically, uses the term "validity" correctly, distinguishes the enquiry type used from an appropriate alternative (randomised controlled comparison would be more appropriate here), and addresses the conclusion's causal overreach.

Ask pupils to annotate their own response:
- Place a tick where their response matches the model.
- Place a cross where they missed a point.
- Underline any specific technical term in the model that they did not use.

This is self-assessment practice that mirrors the GCSE post-trial reflection skills required in Y10/11.

**Phase 4 — Explicit instruction on vocabulary gaps (8 minutes)**
From pupil responses and the retrieval warm-up, identify the two or three vocabulary items most commonly absent or imprecise. Teach these explicitly now, with a definition and two contrasting examples. Common targets at Y9: confounding variable (vs control variable), validity (vs reliability), operational definition.

**Phase 5 — Metacognitive reflection (5 minutes, `metacognitive_reflection`)**
Written, individual. Prompt: "Looking at your response and the model answer, identify the one thing you understood well and the one thing that was absent or imprecise. What would you have needed to know to produce the model answer?"

This is not a feelings check — it is a cognitive gap analysis. Collect and use it to plan differentiation for Lesson 2.

---

### Lesson 2: Enquiry type discrimination — when is a fair test not appropriate? (~50 minutes)

**Session sequence:** challenge_problem → independent_practice → model_answer_comparison → retrieval_practice → metacognitive_reflection

**Learning focus:** Systematic discrimination between enquiry types; understanding what each type can and cannot conclude.

**Phase 1 — Retrieval warm-up (5 minutes, `testing_effect`, `interleaving`)**
Three rapid retrieval questions interleaved across Y7/8 biology and chemistry:
1. Define "independent variable" in one sentence without using the word "change". (Forces precision.)
2. Name two differences between repeatability and reproducibility.
3. [Chemistry context]: A student investigates how the concentration of acid affects the rate of reaction with magnesium ribbon. They measure the volume of gas produced every 30 seconds. Write the operational definition of the dependent variable.

**Phase 2 — Enquiry type classification task (15 minutes, `compare_contrast_table`, `generation_effect`)**

Present pupils with eight scientific scenarios. Their task is to classify each scenario by enquiry type and, crucially, to justify their classification in one sentence.

Example scenarios (designed to expose M1 and M5):

1. A biologist records the number of different insect species in 10 meadows at different distances from a motorway. *(Pattern-seeking — the biologist cannot manipulate distance.)*
2. A chemist tests whether temperature affects the rate of dissolving by using five different temperatures. *(Comparative fair test.)*
3. A student measures the pH of rainwater every day for one year. *(Observational/longitudinal.)*
4. A geographer classifies 50 rock samples collected from different locations by rock type. *(Identifying and classifying.)*
5. A student reads three published papers about the effect of microplastics on coral growth to answer a research question. *(Secondary sources.)*
6. A physiologist measures the resting heart rate of 200 adults of different ages. *(Pattern-seeking.)*
7. A student cuts three different metals into equal-sized strips and places each in the same concentration of acid. *(Comparative fair test.)*
8. A pupil wants to know whether increased exercise causes weight loss. They review data from 10 published longitudinal studies. *(Secondary sources — though a comparative fair test could also be argued if designed as an RCT.)*

After independent completion, use `compare_contrast_table` interaction (in the AI system): a two-column table with rows for each enquiry type. Pupils enter: (a) what it involves, (b) what kind of conclusion it can support, (c) what it cannot conclude. The final row should be blank for pupils to generate their own example of each type.

After completion, AI prompt: "Looking at your table, which enquiry type do you think is most commonly misused, and why?" — pushes toward synthesis beyond recall.

**Phase 3 — The "manipulable vs non-manipulable variable" distinction (10 minutes, direct instruction)**
This is the conceptual linchpin for enquiry type selection. Teach it explicitly:

> A fair test is only appropriate when the independent variable is something the investigator can *deliberately change*. When the independent variable is something the investigator can only *observe or select* — such as age, location, species, or naturally occurring variation — a different enquiry type is needed.

Give three worked examples: temperature (can be manipulated — fair test appropriate), latitude (cannot be manipulated — pattern-seeking appropriate), species of plant (cannot be assigned by the investigator — pattern-seeking or classification appropriate).

**Phase 4 — Independent practice: design critique (10 minutes, `text_input_paragraph`)**
Present a flawed experimental design (this time from physics: "a student investigates whether the length of a pendulum affects the period by timing the pendulum three times and taking the average"). Pupils write: (a) what enquiry type this is; (b) whether the design is appropriate; (c) two specific limitations; (d) one targeted improvement for each limitation.

**Phase 5 — Metacognitive reflection (5 minutes)**
Prompt: "Before today, would you have said that pattern-seeking and fair testing are basically the same thing? What is the single most important distinction between them?"

**Interleaving note:** At some point in Lessons 2–5, retrieve content from SC-KS3-D001-CL007 (data presentation and pattern identification) by asking pupils to specify what graph type they would use for results from each enquiry type and why. This is not the focus of the lesson but reinforces the connection between enquiry design and data representation.

---

### Lesson 3: Productive failure — design your own, then discover what you missed (~50 minutes)

**Session sequence:** challenge_problem → independent_practice → model_answer_comparison → metacognitive_reflection

**Learning focus:** Moving from variable identification to full experimental design; experiencing design failure as a learning tool.

This lesson is the productive failure heart of the sequence. The pedagogical principle is that pupils who generate an imperfect design and then compare it against expert criteria retain the evaluation criteria more robustly than pupils who are given the criteria first. The challenge problem should be difficult enough to produce genuine design attempts with multiple flaws.

**Phase 1 — Generation effect warm-up (5 minutes, `generation_effect`)**
Before any instruction or prompts, ask: "Without looking at any notes, write down everything you know about what makes a scientific investigation valid and reliable. You have three minutes." Collect. Do not discuss yet. This primes the encoding of the subsequent instruction.

**Phase 2 — Challenge problem: design from scratch (20 minutes)**
Present the following stimulus and task:

> *You have been given access to: a school science laboratory, six different metals (magnesium, iron, zinc, copper, aluminium, lead), dilute hydrochloric acid (0.5 mol/dm³), measuring cylinders, thermometers, a stopwatch, a balance, a gas syringe, and safety equipment. Design a full investigation to test the following question: "Is there a relationship between the reactivity of a metal and the rate of its reaction with dilute hydrochloric acid?"*

Task (`text_input_paragraph`, multi-part):
1. State the enquiry type you have selected and justify this choice in one sentence.
2. Write a prediction with a mechanistic justification.
3. Identify the IV, DV, and at least four control variables. For each control variable, explain why it must be controlled (what would happen if it were not).
4. Write an operational definition for your dependent variable.
5. Specify your measurement range and interval for the IV.
6. Identify one variable you cannot fully control and explain how it would affect your conclusion.

This should take most of the lesson. Pupils should work individually. The AI system should not provide hints unless a pupil produces nothing at all after 7 minutes (Tier 1: "What do you know about how metals react with acids? Start there.").

**Phase 3 — Model answer comparison and self-assessment (15 minutes, `model_answer_comparison`)**
Reveal a mark-scheme-structured model answer. This should be at AQA GCSE planning question quality. Key features:
- Enquiry type: comparative fair test (the metals are categorically different — you cannot "change" reactivity continuously, but you can compare different metals — this is a nuanced case that sits between fair test and classification; an excellent answer acknowledges this)
- Prediction: references the reactivity series and electron donation; does not just say "more reactive metals react faster"
- Control variables: all four are listed with specific consequences of not controlling them (not just "it would make it unfair")
- Operational definition: specifies volume of gas collected per unit time, instrument used, measurement frequency
- Acknowledgement of residual confounding: surface area of metal, purity of metal, impurities in acid

Ask pupils to award their own design a mark out of 12 using the mark scheme provided. Then write: "What did I miss, and why?"

**Phase 4 — Metacognitive debrief (10 minutes)**
This is not optional at Y9. The metacognitive reflection prompt should be: "Your design had [specific gap identified from self-marking]. What was the reasoning error that produced this gap — was it a knowledge gap (you did not know this needed to be controlled), a precision gap (you knew but did not specify it clearly enough), or a reasoning gap (you did not think through the consequences of not controlling it)?"

Distinguishing knowledge gaps from reasoning gaps is a Y9-level metacognitive task. It points toward different remediation paths.

**Differentiation:**
- Pupils who produce a substantially complete design: extend by asking them to evaluate the design's external validity — does this investigation tell you anything about how metals react in real-world conditions? What are the limitations of a laboratory study?
- Pupils who struggle to generate a full plan: focus the model answer comparison on the control variables section only. Depth over breadth.

---

### Lesson 4: Prediction and hypothesis — making the distinction stick (~50 minutes)

**Session sequence:** challenge_problem → independent_practice → model_answer_comparison → retrieval_practice → metacognitive_reflection

**Learning focus:** The prediction/hypothesis distinction; mechanistic prediction writing; critique of weak predictions.

**Phase 1 — Retrieval warm-up (7 minutes, `testing_effect`, `interleaving`)**
Interleave across biology (cell biology context), chemistry (rates of reaction), and physics (waves or forces — whichever has been most recently taught):
1. [Biology] Write a prediction for an investigation into the effect of temperature on enzyme activity in amylase. Include a specific expected outcome and a mechanistic justification.
2. [Chemistry] What is the difference between the rate of reaction and the yield of a reaction?
3. [Physics] [Context-specific question from current physics topic — teacher selects.]

**Phase 2 — Direct instruction: hypothesis vs prediction (10 minutes)**
Use paired examples to make the distinction explicit. The following are genuine Y9-level confusions, presented as examples to sort and improve:

*Pair 1:*
A: "I predict that the rate of enzyme activity will decrease at temperatures above 40°C."
B: "My hypothesis is that above 40°C, the increased thermal energy causes the bonds maintaining the enzyme's tertiary structure to vibrate and break, changing the shape of the active site so that substrate molecules no longer fit, reducing or preventing catalysis."

Which is the prediction? Which is the hypothesis? (A is the prediction; B is the hypothesis.) What is the relationship between them? (The hypothesis explains why the prediction is expected — the prediction is what you would observe if the hypothesis is correct.)

*Pair 2:*
A: "My hypothesis is that plants in low light will be shorter."
B: "My prediction is that without sufficient light energy, the rate of photosynthesis is reduced, limiting the production of glucose needed for respiration and growth, so plants in low light will grow more slowly and reach a smaller final height."

Which is the prediction? Which is the hypothesis? (This is deliberately reversed — A uses "hypothesis" but is actually a prediction; B uses "prediction" but provides a mechanistic explanation. The label is wrong on both.) Ask pupils to rewrite each with the correct label and improve the weaker one.

This exercise targets M4 directly: the distinction is not about the word used but about the nature of the claim — observed outcome vs explanatory mechanism.

**Phase 3 — Independent writing task (15 minutes, `text_input_paragraph`)**
Present a new investigation scenario (biology: "Does the colour of light affect the rate of photosynthesis?"). Task: write both a prediction AND a hypothesis for this investigation. They must be distinct and both must be scientifically justified. Minimum 80 words each.

Evaluation criteria for AI feedback (`text_input_paragraph`, criterion-referenced):
- Prediction: is there a specific expected outcome? Is it linked to the IV and DV? Is there a directional claim?
- Hypothesis: is there a proposed mechanism? Does it reference a named scientific process or concept? Is it falsifiable?

**Phase 4 — Peer critique using `inference_task` (10 minutes)**
Present a weak prediction (prepared by teacher, displaying common errors) for Socratic evaluation:

> *"I predict that blue light will make photosynthesis happen more because blue light has more energy."*

AI `inference_task` sequence:
1. "What do you notice about this prediction?"
2. "What does it tell you about what the student understands?"
3. "Why do you think the phrase 'more energy' is imprecise here?"
4. "What would need to be different for this to count as a strong scientific prediction?"

Do not affirm or deny until the end of the chain. Final step: confirm the specific gap — the student's prediction gives an expected direction but provides no mechanistic justification linked to the specific biochemistry of light-dependent reactions (light wavelength and chlorophyll absorption spectra).

**Phase 5 — Retrieval practice (5 minutes, `multi_choice_5`)**
Five multi-choice questions targeting vocabulary precision: hypothesis vs prediction, IV vs DV, validity vs reliability. Each distractor addresses a documented misconception from this cluster (M4, M7). AI explains why each distractor is wrong in post-answer feedback.

**Phase 6 — Metacognitive reflection (3 minutes)**
Prompt: "The distinction between prediction and hypothesis has been on the KS3 curriculum since Y7. Why do you think you had not separated them clearly before? Was it a definition problem or an application problem?"

---

### Lesson 5: Consolidation and exam preparation — from design to critique (~50 minutes)

**Session sequence:** challenge_problem → independent_practice → model_answer_comparison → retrieval_practice → metacognitive_reflection

**Learning focus:** Integrating all four concepts (questioning, prediction, variables, experimental design) in exam-style extended response; self-assessment against mark scheme.

**Phase 1 — Spaced retrieval from earlier in the sequence (8 minutes, `testing_effect`)**
Return to the diagnostic questions from Lesson 1. Do not signal this in advance. Present the same four questions. The aim is to quantify learning gain within the sequence and to demonstrate to pupils that their thinking has become more precise.

After completing them, pupils compare their Lesson 1 responses to their Lesson 5 responses. Teacher circulates to support comparisons. This is a deliberate use of the testing effect: the retrieval itself strengthens retention, and the comparison builds metacognitive awareness of growth.

**Phase 2 — Challenge problem: full exam-style question (25 minutes, `text_input_paragraph`)**
Present a 6-mark GCSE-style planning question:

> *A student claims that the amount of salt dissolved in water affects how quickly an egg sinks when placed in a glass of the solution. Design an investigation to test this claim. In your answer, include: the independent and dependent variables and how each will be measured; a prediction with justification; at least three control variables with reasons why each must be controlled; and a description of how you would assess whether your results are reliable.*

This is a synthesis task. It requires all four concepts simultaneously. No hints for the first 15 minutes. After 15 minutes, if a pupil's written response is fewer than 80 words, offer Tier 1 hint: "Start by identifying what the student is claiming will change when you add more salt."

After 25 minutes, reveal a mark-scheme structured model answer. Pupils self-assess on a six-point scale. AI feedback register (`gcse_exam_style`): "Your response would earn [n] of 6 marks because [specific criteria addressed]. To reach full marks, you would need [specific addition]."

**Phase 3 — Targeted gap filling (7 minutes)**
Based on self-assessment, each pupil identifies their single lowest-scoring section and rewrites it with the mark scheme in view. This is not a copy exercise — they should produce an improved version in their own words.

**Phase 4 — Retrieval practice: interleaved across the domain (7 minutes, `multi_choice_5`, `interleaving`)**
Five questions drawn from across SC-KS3-D001: data presentation (SC-KS3-D001-CL007), accuracy and precision (SC-KS3-D001-CL009), conclusions (SC-KS3-D001-CL011), plus two from this cluster. The interleaving is deliberately heterogeneous to prevent blocked-practice retrieval shortcuts.

**Phase 5 — Metacognitive reflection (3 minutes)**
Prompt: "Which of the four skills in this cluster — questioning, prediction, variable identification, experimental design — do you find most difficult to apply in an unfamiliar context? What is the specific source of difficulty?"

The purpose of specificity here is to distinguish "I find it hard because I don't know the vocabulary" from "I find it hard because I don't know which strategy to start with" — these require different responses.

---

## 8. Assessment and Feedback

### Formative assessment strategy

**Distinguishing enquiry type knowledge from variable control procedural knowledge:**
These are separate competencies and should be assessed separately. A pupil can correctly identify that an investigation requires a fair test while still failing to identify all necessary control variables — or vice versa. Use distinct questions for each, and do not allow a strong performance on one to mask a gap on the other.

Diagnostic checkpoints across the sequence:

| Lesson | Assessment focus | Method |
|---|---|---|
| 1 | Baseline: all four concepts | Written diagnostic (Phase 2), collected, not marked publicly |
| 2 | Enquiry type discrimination | Self-assessment against model after classification task |
| 3 | Full design quality: control variables, operational definition | Mark scheme self-assessment (out of 12) |
| 4 | Prediction vs hypothesis distinction | Criterion-referenced written response + `multi_choice_5` |
| 5 | Integration: exam-style | Mark scheme self-assessment (out of 6) + spaced retrieval comparison |

### Feedback language: Y9 register

The Y9 feedback profile is `gcse_exam_style` with `examination_coach` tone. The following guidance is binding on both teacher and AI system feedback.

**When a response is correct:**
Do not praise performance. Extend the thinking. Examples:
- "Your response addresses all three criteria. The extension: your operational definition specifies the instrument but not the measurement range. At what salt concentration would you stop, and why?"
- "Full marks on variable identification. The question this raises: if you cannot fully control the temperature of the solution, how does that affect the conclusion you can draw?"
- "Your prediction is justified. Consider: what result would falsify your prediction? If you cannot answer that, the prediction may not be fully testable."

**When a response is incorrect or incomplete:**
Reference the mark scheme structure. Be specific about which criterion is missing. Examples:
- "This response would earn 2 of 6 marks. You have identified the IV and DV correctly and specified units — that satisfies criteria 1 and 2. Missing: (a) operational definition of the DV (criterion 3), (b) reasons for control variables, not just their names (criterion 4), (c) a specific measure of reliability (criterion 6). Revise in light of criteria 3–4 first."
- "Your prediction gives the expected direction but no mechanism. A GCSE examiner would award 1 mark here, not 2. Add a sentence that references the specific scientific process — why does increasing salt concentration affect sinking time? What is happening to the water at the particle level?"

**Phrases to avoid — always:**
"Amazing", "Brilliant", "Well done", "That is clever"

**Also avoid at Y9 specifically:**
"Good try", "Nearly there", "You almost got it" — these do not give information. Replace with the specific criterion not yet met.

### Metacognitive reflection prompts (Y9-appropriate)

These should appear at the end of every session. Rotate through:

- "Identify the specific reasoning error in your Lesson 1 response that you would not make now. What changed?"
- "Which of the five enquiry types do you find it hardest to distinguish from a fair test? What is the specific conceptual difficulty?"
- "If you were designing the assessment for this cluster, which concept would you test with a 6-mark question? Why?"
- "What aspect of experimental design do you think is most often undervalued by GCSE pupils? How would you address that in an exam answer?"
- "Your model answer comparison showed a gap in [specific area]. Was this a knowledge gap, a reasoning gap, or a communication gap? What is the difference, and what is the appropriate response to each?"

---

## 9. Adaptive Learning Notes (AI System)

### Adaptation if pupils conflate enquiry types

If, after Lesson 2, a pupil consistently applies "fair test" framing to pattern-seeking or observational scenarios, the AI system should:

1. Present the "manipulable vs non-manipulable variable" prompt explicitly: "Can the investigator in this scenario actually change [identified IV], or can they only observe different naturally occurring levels of it?"
2. Provide a contrasting pair: one scenario that is correctly a fair test and one that is correctly pattern-seeking, with the same surface features (same science topic, different variable type). Ask the pupil to identify the difference.
3. If confusion persists after two contrasting pairs, escalate to Tier 3 hint: provide the full classification table with the key criterion for each type.

Do not re-teach the full five-type taxonomy from scratch — the issue is typically discriminating fair test from pattern-seeking, not confusion about all five types.

### Hint tier strategy for this cluster at Y9

Maximum 3 hint tiers (Y9 profile). Apply as follows:

| Tier | Prompt type | When to deploy |
|---|---|---|
| 1 | Conceptual frame question | After 7 minutes with no attempt or response fewer than 40 words: "What approach might you take? Start by identifying what the investigation is asking you to change or observe." |
| 2 | Subject-specific method cue | After Tier 1 produces no substantive progress: name the specific concept the question requires (e.g., "This question requires an operational definition — a specification of exactly how you would measure [DV]") |
| 3 | Model answer for comparison | After Tier 2: reveal the model answer section relevant to the gap, not the full answer. Ask pupil to identify where their response differs |

The Tier 3 reveal is specifically for novel or high-demand content, not for concepts that should be secure at Y9. If a Y9 pupil requires Tier 3 hint for variable identification, flag this for targeted intervention.

### Spaced retrieval schedule

Spacing interval for Y9: 7–35 days. Following this cluster, the AI system should schedule retrieval questions from SC-KS3-D001-CL002 at:

- Day 7: 3 retrieval questions (variable identification, enquiry type selection, operational definition writing) — `multi_choice_5` and `text_input_sentence`
- Day 14: 2 retrieval questions interleaved with questions from SC-KS3-D001-CL007 (data presentation) — connecting enquiry design to data analysis
- Day 21: 1 exam-style question requiring full investigation planning — `text_input_paragraph`, evaluated against mark scheme
- Day 35: Integration retrieval with SC-KS3-D001-CL009 (accuracy, precision, error) — evaluation of a described method

Retrieval-only: no re-teaching unless mastery drops below 80% (4 of 5 questions incorrect in a retrieval session). If it does, escalate to teacher review rather than automated re-delivery of content.

### Desirable difficulties in use

| Technique | Implementation in this cluster |
|---|---|
| **Spacing** | Retrieval questions from Y7/8 Working Scientifically content embedded in every lesson warm-up; scheduled retrieval at 7, 14, 21, 35 days post-cluster |
| **Interleaving** | Every retrieval warm-up mixes biology, chemistry, and physics surface contexts; Lesson 5 retrieval practice draws from four different domain clusters |
| **Varied practice** | Same underlying skill (variable identification, enquiry type selection) presented across six different science contexts (photosynthesis, enzymes, metal reactivity, pendulums, salt solutions, heart rate) |
| **Generation effect** | Pupils retrieve rules and definitions from memory before they are shown; design tasks require generation of original investigation plans before model comparison |
| **Testing effect** | Lesson 5 repeats Lesson 1 diagnostic questions as retrieval practice; all previously mastered concepts (from Y7/8 sessions) receive retrieval-only practice with no re-teaching |

---

## 10. Resources

### Laboratory resources
- Marble chips and dilute hydrochloric acid (Lesson 3 design context — not necessarily practical, used as a planning stimulus)
- Variety of metals (or photographs/data tables if no practical is conducted)
- Thermometers, measuring cylinders, gas syringes, stopwatch, balance (as listed in Lesson 3 design brief)

### Printed / projected resources
- Investigation planning framework: Question → Prediction → Hypothesis → IV → DV → Control variables (with "why must it be controlled?") → Operational definition of DV → Measurement range and interval → Sample size or number of replicates → Risk assessment
- The five enquiry types reference card (for Lesson 2 — available as reference, not as a teaching prop during Phase 2 classification task; reveal after independent attempt)
- Mark scheme for 6-mark planning question (Lesson 5, Phase 2)
- Paired prediction/hypothesis examples (Lesson 4)

### Secondary sources
- GCSE science past papers (AQA, OCR, or Edexcel, depending on exam board): method design and evaluation questions are common in all papers, typically 4–6 marks
- CLEAPSS resources for risk assessment support
- IOPSpark (for physics investigation contexts) and Nuffield Foundation (for biology and chemistry contexts)

### AI interaction types (by lesson)

| Lesson | Primary interaction types | Notes |
|---|---|---|
| 1 | `text_input_paragraph`, `multi_choice_5` | Extended evaluative response; no scaffolding by default |
| 2 | `compare_contrast_table`, `text_input_paragraph` | Table population without pre-supplied criteria (Y9 level) |
| 3 | `text_input_paragraph` | Multi-part design question; hint tiers only after 7 minutes silence |
| 4 | `text_input_paragraph`, `inference_task`, `multi_choice_5` | Socratic chain for weak prediction critique; no affirmation until chain complete |
| 5 | `text_input_paragraph`, `multi_choice_5` | Exam-style response followed by interleaved retrieval |

---

## 11. Links to Next Learning

### Immediate next cluster
`SC-KS3-D001-CL003` — Assessment cluster: Scientific questioning, Scientific prediction, Variables in experiments (+1). This assessment cluster follows directly. The skills formalised in this teaching sequence are what the assessment measures. Preparation should include a retrieval session at day 7 using the spaced schedule above.

### Within the Working Scientifically domain
This cluster is the planning stage. The subsequent practice clusters build the full cycle:
- **CL004** (Laboratory equipment and techniques) — the design skills from CL002 are applied to equipment selection and practical skills
- **CL005** (Measurement, SI units, equations) — the operational definition of the DV connects directly to measurement precision
- **CL007** (Data presentation and pattern identification) — the choice of enquiry type determines what graph type is appropriate; pattern-seeking requires scatter graphs and correlation analysis, not bar charts
- **CL009** (Accuracy, precision, error) — the validity and reliability concepts introduced here are the conceptual foundation for evaluating data quality
- **CL011** (Conclusions and further questions) — the prediction/hypothesis distinction set up here is directly tested in conclusion writing: does the result support the hypothesis?

### KS4 required practicals
All GCSE science exam boards include required practicals (AQA) or core practicals (Edexcel/OCR). In every case, written paper method questions expect pupils to:
- Identify and justify choice of variables
- Explain control of specific variables with reference to validity
- Write operational definitions of measured variables
- Evaluate the reliability and validity of a described method
- Suggest targeted improvements

The vocabulary, reasoning structures, and mark scheme awareness developed in this cluster are exactly what those questions require.

### CPAC (A-level Practical Competency Assessment)
For pupils proceeding to A-level sciences, the Common Practical Assessment Criteria require documented evidence of: following and adapting experimental procedures, applying investigative approaches, and analysing and evaluating data. All three require the investigation design literacy developed in this cluster.

### The investigation design literacy transfer problem
One documented issue at the KS3/4 boundary is that pupils who can correctly identify variables in a prescribed context cannot transfer this skill to a novel investigation they have designed themselves. This cluster, taught at Y9, is specifically designed to address this transfer gap by requiring original design (Lesson 3), critique of designs (Lessons 1 and 3), and evaluation in unfamiliar surface contexts (Lessons 2, 4, and 5). The productive failure model in Lesson 3 is the highest-leverage single activity for building transfer, because the comparison of personal design against expert criteria is more effective than instruction alone.

Pupils who complete this cluster with strong performance on Lesson 5's exam-style question should be well-positioned for GCSE method questions. Pupils who remain insecure on enquiry type discrimination or on the validity/reliability distinction should be identified for targeted retrieval intervention in the first half of Y10.
